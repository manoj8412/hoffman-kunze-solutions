\documentclass{article}
\usepackage{array}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{
    bookmarksnumbered=true,
    pdfpagemode=UseOutlines,
}

\input{util.tex}

\begin{document}

\setcounter{section}{2}
\section{Linear Transformations}

\subsection{Linear Transformations}
\begin{enumerate}[listparindent=\parindent]
\item[1.] Which of the following functions \(T\) from \(R^2\) into \(R^2\) are linear transformations?
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] \(T(x_1, x_2) = (1 + x_1, x_2)\);
            \[ T((0, 0)) = (1, 0) \neq (0, 0)\]
            \(T\) is not a linear transformation.
        \item[(b)] \(T(x_1, x_2) = (x_2, x_1)\);
            \begin{gather*}
                T(c(x_1, x_2) + (y_1, y_2)) = \\
                T((cx_1 + y_1, cx_2 + y_2)) = \\
                (cx_2 + y_2, cx_1 + y_1)) = \\
                c(x_2 + x_1) + (y_2, + y_1)
            \end{gather*}
            \(T\) is a linear transformation.
        \item[(c)] \(T(x_1, x_2) = (x_1^2, x_2)\);
            \begin{gather*}
                T(cx_1 + y_1, cx_2 + y_2) = \\
                ((cx_1 + y_1)^2, cx_2 + y_2) = \\
                (c^2x_1^2 + 2cx_1y_1 + y_1^2, cx_2 + y_2) \neq \\
                c(x_1^2, x_2) + (y_1^2, y_2)
            \end{gather*}
            \(T\) is not a linear transformation.
        \item[(d)] \(T(x_1, x_2) = (\sin x_1, x_2)\);
            \begin{gather*}
                T(cx_1 + y_1, cx_2 + y_2) = \\
                (\sin(cx_1 + y_1), cx_2 + y_2) \neq \\
                c(\sin x_1, x_2) + (\sin y_1, y_2)
            \end{gather*}
            \(T\) is not a linear transformation.
        \item[(e)] \(T(x_1, x_2) = (x_1 - x_2, 0)\);
            \begin{gather*}
                T(c(x_1, x_2) + (y_1, y_2)) = \\
                (cx_1 + y_1 - cx_2 - y_2, 0) = \\
                (c(x_1 - x_2) + y_1 - y_2, 0)
            \end{gather*}
            \(T\) is a linear transformation.
    \end{enumerate}

\item[2.] Find the range, rank, null space, and nullity for the zero transformation and the identity transformation
    on a finite-dimensional space \(V\).

    For the zero transformation, the range is \(\{0\}\), the rank is 0, the null space is \(V\), and the nullity is \(\dim V\).
    For the identity transformation, the range is \(V\), the rank is \(\dim V\), the null space is \(\{0\}\), and the nullity is 0.

\item[4.] Is there a linear transformation \(T\) from \(R^3\) into \(R^2\) such that \(T(1, -1, 1) = (1, 0)\) and \(T(1, 1, 1) = (0, 1)\)?

    \((1, -1, 1)\) and \((1, 1, 1)\) are linearly independent, so there exists a basis of \(R^3\) that contains the two vectors.
    Then by Theorem 1, there exists a linear transformation that maps this basis into the desired outputs.

\item[5.] If
    \begin{gather*}
        \alpha_1 = (1, -1),\quad \beta_1 = (1, 0) \\
        \alpha_2 = (2, -1),\quad \beta_2 = (0, 1) \\
        \alpha_3 = (-3, 2),\quad \beta_3 = (1, 1)
    \end{gather*}
    is there a linear transformation \(T\) from \(\mathbb{R}^2\) into \(\mathbb{R}^2\) such that \(T\alpha_1 = \beta_i\) for \(i = 1, 2, 3\)?

    If such \(T\) exists, then \(T\alpha_3 = -(T\alpha_1 + T\alpha_2)\),
    but \(-(T\alpha_1 + T\alpha_2) = -(\beta_1 + \beta_2) = -\beta_3\) so \(T\alpha_3 \neq \beta_3\).
    Therefore there is no linear transformation with the given properties.

\item[6.] Describe explicitly (as in Exercises 1 and 2) the linear transformation \(T\) from \(F^2\) into \(F^2\)
    such that \(T\epsilon_1 = (a, b)\) and \(T\epsilon_2 = (c, d)\).

    \[T(x_1, x_2) = (ax_1 + cx_2, bx_1 + dx_2)\]

\item[8.] Describe explicitly a linear transformation from \(\mathbb{R}^3\) into \(\mathbb{R}^3\)
    which has its range the subspace spanned by \((1, 0, -1)\) and \((1, 2, 2)\).

    Let \(T\epsilon_1 = (1, 0, -1), T\epsilon_2 = (1, 2, 2), T\epsilon_3 = (0, 0, 0)\), then
    \[T(x_1, x_2, x_3) = (x_1 + x_2, 2x_2, -x_1 + 2x_2) = x_1(1, 0, -1) + x_2(1, 2, 2)\]

\item[9.] Let \(V\) the vector space of all \(n \times n\) matrices over the field \(F\), and let \(B\) be a fixed \(n \times n\) matrix.
    If \(T(A) = AB - BA\) verify that \(T\) is a linear transformation from \(V\) to \(V\).

    If \(X\) and \(Y\) are some \(n \times n\) matrices.
    \begin{gather*}
        T(cX + Y) = (cX + Y)B - B(cX + Y) \\
        T(cX + Y) = cXB + YB - cBX - BY \\
        T(cX + Y) = cXB - cBX + YB - BY \\
        T(cX + Y) = cT(X) + cT(Y)
    \end{gather*}

\item[10.] Let \(V\) the set of all complex numbers regarded as a vector space over the field of \textit{real} numbers (usual operations).
    Find a function from \(V\) to \(V\) which is a linear transformation on the above vector space, but which is not a linear transformation on \(\mathbb{C}^1\),
    i.e., which is not complex linear.

    Let \(T(x + yi) = x - yi\). If \(c\) is real, then \[(T(c(x + yi)) = cx - cyi = c(x - yi) = cT(x + yi),\]
    but \[T(i(i)) = T(-1) = -1 \neq 1 = i(-i) = iT(i)\]

\item[11.] Let \(V\) be the space of \(n \times 1\) matrices over \(F\) and let \(W\) be the space of \(m \times 1 \) matrices over \(F\).
    Let \(A\) be a fixed \(m \times n\) over \(F\) and let \(T\) be the linear transformation from \(V\) to \(W\) defined by \(T(X) = AX\).
    Prove that \(T\) is the zero transformation if and only if \(A\) is the zero matrix.

    \(AX\) is some linear combination of the column vectors of \(A\); \(AX = \alpha_1x_1 + \dots \alpha_nx_n\) where \(\alpha_i\) are columns of \(A\).
    This is only equal to the zero vector for any arbitrary \(X\) if and only if \(A = 0\).

\item[12.] Let \(V\) be an \(n\)-dimensional vector space over the field \(F\) and let \(T\) be a linear transformation from \(V\) into \(V\)
    such that the range and null space of \(T\) are identical. Prove that \(n\) is even. (Can you give an example of such a linear transformation \(T\)?)

    If the range and the null space of \(T\) are identical, then their dimensions are the same.
    \begin{gather*}
        \rank T + \nullity T = \dim V \\
        \rank T + \rank T = n \\
        2(\rank T) = n
    \end{gather*}
    Therefore \(n\) is even.

    For the space of \(2 \times 1\) matrices, the linear transformation \[T(A) = \begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix}A\] is one such transformation.
    The range of \(T\) is spanned by \((1, 0)\), and \(AX = 0\) if and only if \(X\) is in the form \(\begin{bmatrix} a \\ 0 \end{bmatrix}\) so the null space is spanned by \((1, 0)\) as well.

\item[13.] Let \(V\) be a vector space and \(T\) a linear transformation from \(V\) into \(V\). Prove that the two following statements about \(T\) are equivalent.
    \begin{enumerate}
        \item[(a)] The intersection of the range of \(T\) and the null space of \(T\) is the zero subspace of \(V\).
        \item[(b)] If \(T(T\alpha) = 0\), then \(T\alpha = 0\).
    \end{enumerate}

    Suppose statement (a) is true. If \(T(T\alpha) = 0\), it must be the case that \(T\alpha\) is in the null space of \(T\).
    Since it is also clearly in its range, \(T\alpha\) must be 0.

    Suppose statement (b) is true; \(T\alpha\) is in the range and the null space of \(T\), and \(T\alpha = 0\).
    The zero vector is the only vector that is both in the range and the null space, so the intersection of the range and null space is the zero subspace.
\end{enumerate}

\subsection{The Algebra of Linear Transformations}
\begin{enumerate}[listparindent=\parindent]
\item[1.] Let \(T\) and \(U\) be the linear operations on \(\mathbb{R}^2\) defined by
    \[ T(x_1, x_2) = (x_2, x_1) \text{ and } U(x_1, x_2) = (x_1, 0). \]

    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] How would you describe \(T\) and \(U\) geometrically?

            \(T\) is a reflection across the line \(y = x\), and \(U\) is projection onto the x-axis.

        \item[(b)] Give rules like the ones defining \(T\) and \(U\) for each of the transformations
            \((U + T), UT, TU, T^2, U^2\).

            \[(U + T)(x_1, x_2) = (x_1 + x_2, x_1)\]
            \[UT(x_1, x_2) = (x_2, 0)\]
            \[TU(x_1, x_2) = (0_1, x_1)\]
            \[T^2(x_1, x_2) = (x_1, x_2)\]
            \[U^2(x_1, x_2) = (x_1, 0)\]
    \end{enumerate}

\item[2.] Let \(T\) be the (unique) linear operator on \(\mathbb{C}^3\) for which
    \[ T\epsilon_1 = (1, 0, i),\quad T\epsilon_2 = (0, 1, 1),\quad T\epsilon_3 = (i, 1, 0)\]
    Is \(T\) invertible?

    The matrix
    \[
        \begin{bmatrix}
            1 & 0 & i \\
            0 & 1 & 1 \\
            i & 1 & 0
        \end{bmatrix}
    \]
    is not invertible so \(\{T\epsilon_1, T\epsilon_2, T\epsilon_3\}\) are linearly dependent and cannot be a basis of \(\mathbb{C}^3\).
    Therefore \(T\) is not invertible.

\item[3.] Let \(T\) be the linear operator on \(\mathbb{R}^3\) defined by
    \[T(x_1, x_2, x_3) = (3x_1, x_1 - x_2, 2x_1 + x_2 + x_3).\]
    Is \(T\) invertible? If so, find a rule for \(T^{-1}\) like the one which defines \(T\).

    If \(T(x_1, x_2, x_3) = 0\), then
    \[
        \begin{system}{3}
            3x_1 && && &=& 0 \\
            x_1 &-& x_2 && &=& 0 \\
            2x_1 &+& x_2 &+& x_3 &=& 0 \\
        \end{system}
    \]
    \(x_1 = x_2 = x_3\), so \(T\) is non-singular and therefore invertible.

    \[
        \begin{system}{3}
            3y_1 && && &=& x_1 \\
            y_1 &-& y_2 && &=& x_2 \\
            2y_1 &+& y_2 &+& y_3 &=& x_3 \\
        \end{system}
    \]

    Solve this system to find
    \[ T^{-1}(x_1, x_2, x_3) = (\frac{x_1}{3}, \frac{x_1}{3} - 2, -x_1 + x_2 + x_3) \]

\item[5.] Let \(\mathbb{C}^{2 \times 2}\) be the complex vector space of \(2 \times 2\) matrices with complex entries.
    Let \[ B = \begin{bmatrix} 1 & -1 \\ -4 & 4 \end{bmatrix} \] and let \(T\) be the linear operator on \(\mathbb{C}^{2 \times 2}\)
    defined by \(T(A) = BA\). What is the rank of \(T\)? Can you describe \(T^2\)?

    \(B\bigl[\begin{smallmatrix} a & c \\ b & d \end{smallmatrix}\bigr] = 0\) implies \(a = b\) and \(c = d\), so the null space of \(T\) is spanned by two vectors:
    \begin{gather*}
        B\begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \\
        B\begin{bmatrix} 0 & 1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}
    \end{gather*}
    Therefore \(\nullity(B)= 2\). Since \(\dim \mathbb{C}^{2 \times 2} = 4\), \(\rank(B)= 2\).

    \[ T^2(A) = BBA = \begin{bmatrix} 5 & -5 \\ -20 & 20 \end{bmatrix}A \]

\item[6.] Let \(T\) be a linear transformation from \(\mathbb{R}^3\) into \(\mathbb{R}^2\),
    and let \(U\) be a linear transformation from \(\mathbb{R}^2\) into \(\mathbb{R}^3\).
    Prove that the transformation \(UT\) is not invertible. Generalize the theorem.

    The range of \(T\) is a subspace of \(\mathbb{R}^2\), implying \(\rank(T) \leq 2\).
    \begin{gather*}
        \nullity(T) = 3 - \rank(T) \\
        \nullity(T) \geq 1
    \end{gather*}

    The inverse of \(UT\) is \(T^{-1}U^{-1}\), but \(T^{-1}\) does not exist and \(UT\) is not invertible.

    To generalize, let \(T\) be a linear transformation from \(V\) into \(W\) and
    let \(S\) be a linear transformation from \(W\) into \(V\).
    where \(V\) and \(W\) are vector spaces with \(\dim V > \dim W\).
    Then \(TS\) is not invertible.

\item[7.] Find two linear operators \(T\) and \(U\) on \(\mathbb{R}^2\) such that \(TU = 0\) but \(UT \neq 0\).

    \(T\) must not be invertible, as the null space of \(T\) is the range of \(U\) implying \(U = UT = 0\).
    Similarly, \(U\) must not be invertible as it implies the null space of \(T\) is \(\mathbb{R}^2\) and \(T = UT = 0\).

    \[T(x, y) = (x - y, x - y), \quad U(x, y) = (x + y, x + y)\]

\item[8.] Let \(V\) be a vector space over the field \(F\) and \(T\) a linear operator on \(V\).
    If \(T^2 = 0\), what can you say about the relation of the range of \(T\) to the null space of \(T\)?
    Give an example of a linear operator \(T\) such that \(T^2 = 0\) but \(T \neq 0\).

    The range of \(T\) is its null space.
    \[ T(x, y) = (0, x) \]

\item[9.] Let \(T\) be a linear operator on the finite-dimensional space \(V\).
    Suppose there is a linear operator \(U\) on \(V\) such that \(TU = I\).
    Prove that \(T\) is invertible and \(U = T^{-1}\).
    Give an example which shows that this is false when \(V\) is not finite-dimensional.
    (\textit{Hint:} Let \(T = D\), the differentiation operator on the space of polynomials.)

    If \(TU = I\), the null space of \(TU\) is the zero subspace since \(TUx = 0\) if and only if \(x = 0\).
    Therefore by Theorem 9, \(TU\) has the range \(V\) and is invertible.
    The range of \(T\) must also be \(V\), so it is invertible as well. Then
    \begin{gather*}
        TU = I \\
        T^{-1}TU = T^{-1} \\
        U = T^{-1}
    \end{gather*}

    Let \(V\) be the space of polynomials, \(D\) the differentiation operator, and \(E\) the indefinite integral operator.
    Then \(DE = I\), but \(ED \neq I\).

\item[11.] Let \(V\) be a finite-dimensional vector space and let \(T\) be a linear operator on \(V\).
    Suppose that \(\rank(T^2) = \rank(T)\). Prove that the range and null space of \(T\) are disjoint, i.e.,
    have only the zero vector in common.

    The null space of \(T\) is a subset of the null space of \(T^2\) because for any vector \(z\) in the null space of \(T\), \(T^2z = T0 = 0\).
    \(\rank(T^2) = \rank(T)\) implies \(\nullity(T^2) = \nullity(T)\), so the null space of \(T\) is identical to the null space of \(T^2\).

    Suppose there exists a nonzero vector \(v\) such that \(v\) is in both the range and the null space of \(T\).
    There exists a nonzero vector \(u\) such that \(Tu = v\). Since \(T^2u = T(Tu) = Tv = 0\), \(u\) is in the null space of \(T^2\).
    But we proved the two operations have the same null space, so \(Tu = v = 0\) which is a contradiction.
    Therefore, the only vector both in the range and the null space of \(T\) must be the zero vector.
\end{enumerate}

\subsection{Isomorphisms}
\begin{enumerate}[listparindent=\parindent]
\item[1.] Let \(V\) be the set of complex numbers and let \(F\) be the field of real numbers.
    With the usual operations, \(V\) is a vector space over \(F\). Describe explicitly an isomorphism of this space onto \(\mathbb{R}^2\).
    \[ T(a + bi) = (a, b) \]

\item[2.] Let \(V\) be a vector space over the field of complex numbers, and suppose there is a isomorphism \(T\) of \(V\) onto \(\mathbb{C}^3\).
    Let \(\alpha_1, \alpha_2, \alpha_3, \alpha_4\) be vectors in \(V\) such that
    \begin{gather*}
        T\alpha_1 = (1, 0, i),\quad T\alpha_2 = (-2, 1 + i, 0), \\
        T\alpha_3 = (-1, 1, 1),\quad T\alpha_4 = (\sqrt{2}, i, 3).
    \end{gather*}
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Is \(\alpha_1\) in the subspace spanned by \(\alpha_2\) and \(\alpha_3\)?

            Since \(T\) is an isomorphism, this is equivalent to asking whether \(T\alpha_1\) is in the span of \(\{T\alpha_2, T\alpha_3\}\).
            \[
                \begin{abmatrix}{2}{1}
                    -2 & -1 & 1 \\
                    1 + i & 1 & 0 \\
                    0 & 1 & i
                \end{abmatrix}
            \]
            reduces into
            \[
                \begin{abmatrix}{2}{1}
                    1 & 0 & \frac{-1}{2}(1 + i) \\
                    0 & 1 & i \\
                    0 & 0 & 0
                \end{abmatrix}
            \]

            So
            \[ T\alpha_1 = \frac{-1}{2}(1 + i)T\alpha_2 + iT\alpha_3 \]
            and \(\alpha_1\) is in the span of \(\{\alpha_2, \alpha_3\}\).

        \item[(b)] Let \(W_1\) be the subspace spanned by \(\alpha_1\) and \(\alpha_2\)
            and let \(W_2\) be the subspace spanned by \(\alpha_3\) and \(\alpha_4\).
            What is the intersection of \(W_1\) and \(W_2\)?
            \begin{gather*}
                aT\alpha_1 + bT\alpha_2 = cT\alpha_3 + dT\alpha_4 \\
                aT\alpha_1 + bT\alpha_2 - cT\alpha_3 - dT\alpha_4 = 0
            \end{gather*}
            Solve this with a matrix with columns \(T\alpha_i\);
            \begin{gather*}
                \begin{bmatrix}
                    1 & 0 & i & 0 \\
                    0 & 1 & \frac{1}{2}(i - 1) & 0 \\
                    0 & 0 & 0 & 1
                \end{bmatrix}
                \begin{bmatrix} a \\ b \\ -c \\ -d \end{bmatrix}
                =
                \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} \\
                iT\alpha_1 + \frac{i - 1}{2}T\alpha_2 - T\alpha_3 = 0
            \end{gather*}
            \(T\alpha_3\) is in the subspace mapped from \(W_1\), implying \(\alpha_3 \in W_1 \cap W_2\).

            The row-reduced matrix shows \(\dim(W_1 + W_2) = 3\), so \[ \dim(W_1 \cap W_2) = \dim(W_1) + \dim(W_2) - \dim(W_1 + W_2) = 2 + 2 - 3 = 1 \]
            Therefore the intersection of \(W_1\) and \(W_2\) is \(\Span (\alpha_3)\).

        \item[(c)] Find a basis for the subspace of \(V\) spanned by the four vectors \(\alpha_i\).

            Part b showed that the four vectors span a subspace with dimension 3, and \(\alpha_3 \in \Span\{\alpha_1, \alpha_2\}\).
            Therefore \(\{\alpha_1, \alpha_2, \alpha_4\}\) is a basis for the given subspace.

    \end{enumerate}

\item[3.] Let \(W\) be the set of all \(2 \times 2\) complex Hermitian matrices, that is,
    the set of \(2 \times 2\) complex matrices \(A\) such that \(A_{ij} = \overline{A_{ji}}\) (the bar denoting complex conjugation)
    As we pointed out in the Example 6 of Chapter 2, \(W\) is a vector space over the field of \textit{real} numbers, under the usual operations.
    Verify that
    \[ (x, y, z, t) \rightarrow \begin{bmatrix} t + x & y + iz \\ y - iz & t - x \end{bmatrix} \]
    is an isomorphism from \(\mathbb{R}^4\) onto \(W\).

    Let \(T\) be the transformation in question. \(T\) is linear:
    \begin{gather*}
        T(c(x, y, z, t)) = \begin{bmatrix} c(t + x) & c(y + iz) \\ c(y - iz) & c(t - x) \end{bmatrix} = cT(x, y, z, t) \\
        T((x_1, y_1, z_1, t_1) + (x_2, y_2, z_2, t_2)) \\
        = \begin{bmatrix} t_1 + t_2 + x_1 + x_2 & y_1 + y_2 + i(z_1 + z_2) \\ y_1 + y_2 - i(z_1 + z_2) & t_1 + t_2 - (x_1 + x_2) \end{bmatrix} \\
        = \begin{bmatrix} t_1 + x_1 & y_1 + iz_1 \\ y_1 - iz_1 & t_1 - x_1 \end{bmatrix} + \begin{bmatrix} t_2 + x_2 & y_2 + iz_2 \\ y_2 - iz_2 & t_2 - x_2 \end{bmatrix} \\
        = T(x_1, y_1, z_1, t_1) + T(x_2, y_2, z_2, t_2)
    \end{gather*}

    The null space of \(T\) is the zero subspace. If \(T(x, y, z, t) = 0\), then \(z = 0\), \(y = 0\), and \(t + x = t - x = 0\) implying \(t = x = 0\).
    Since \(\dim(W) = \dim(\mathbb{R}^4) = 4\), by Theorem 9 \(T\) is bijective and \(T\) is an isomorphism.

\item[4.] Show that \(F^{m \times n}\) is isomorphic to \(F^{mn}\).
    \[
        T\begin{bmatrix}
            a_{11} & \dots & a_{1n} \\
            \vdots & \ddots & \vdots \\
            a_{m1} & \dots & a_{mn} \\
        \end{bmatrix}
        =
        (a_{11}, \dots, a_{1n}, \dots, a_{m1}, \dots, a_{mn})
    \]

    \(T\) is clearly a invertible linear transformation and \\
    \(\dim(F^{m \times n}) = \dim(F^{mn}) = mn\), so the spaces are isomorphic.

\item[6.] Let \(V\) and \(W\) be finite-dimensional vector spaces over the field \(F\). Prove that \(V\) and \(W\) are isomorphic if and only if \(\dim V = \dim W\).

    Suppose \(\dim V = \dim W = n\). There exists a basis \(\{\alpha_1, \dots, \alpha_n\}\) for \(V\) and \(\{\beta_1, \dots, \beta_n\}\) for \(W\).
    Let \(T\) be a linear transformation such that \(T\alpha_i = \beta_i\). \(\{T\alpha_1, \dots, T\alpha_n\}\) is a basis for \(W\), so \(T\) must be invertible.
    Therefore the two spaces are isomorphic.

    If \(V\) and \(W\) are isomorphic, there exists an invertible linear transformation \(U\) from \(V\) onto \(W\).
    By Theorem 8, \(U\) maps a basis of \(V\) into some linearly independent subset of \(W\), so \(\dim V \leq \dim W\).
    \(U^{-1}\) maps a basis of \(W\) into some linearly independent subset of \(V\), so \(\dim V \geq \dim W\).
    Then \(\dim V = \dim W\).

\item[7.] Let \(V\) and \(W\) be vector spaces over the field \(F\) and let \(U\) be an isomorphism of \(V\) onto \(W\).
    Prove that \(T \rightarrow UTU^{-1}\) is an isomorphism of \(L(V, V)\) onto \(L(W, W)\).

    Let \(fT = UTU^{-1}\). Then \(f^{-1}T = U^{-1}TU\) since \[ f^{-1}fT = f^{-1}(UTU^{-1}) = U^{-1}UTU^{-1}U = ITI = T \] and similarly \(ff^{-1} = I\).
    Also, \(f\) is linear. Let \(c \in F\) and \(S \in L(V, V)\).
    \begin{gather*}
        f(cT + S) = U(cT + S)U^{-1} = U(cTU^{-1} + SU^{-1}) \\
        = cUTU^{-1} + USU^{-1} = cfT + cfS
    \end{gather*}
    Therefore \(f\) is an isomorphism.

\end{enumerate}

\subsection{Linear Transformations}
\begin{enumerate}[listparindent=\parindent]

\item[1.] Let \(T\) be the linear operator on \(\mathbb{C}^2\) defined by \(T(x_1, x_2) = (x_1, 0)\).
    Let \(\mathcal{B}\) be the standard ordered basis for \(\mathbb{C}^2\)
    and let \(\mathcal{B}'\) be the ordered basis defined by \(\alpha_1 = (1, i), \alpha_2 = (-i, 2)\).
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] What is the matrix of \(T\) relative to the pair \(\mathcal{B}, \mathcal{B}'\)?
            \[
                \begin{bmatrix} 1 & -i \\ i & 2 \end{bmatrix}^{-1}
                \begin{bmatrix} T\epsilon_1 & T\epsilon_2 \end{bmatrix}
                =
                \begin{bmatrix} 2 & i \\ -i & 1 \end{bmatrix}
                \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
                =
                \begin{bmatrix} 2 & 0 \\ -i & 0 \end{bmatrix}
            \]
        \item[(b)] What is the matrix of \(T\) relative to the pair \(\mathcal{B}', \mathcal{B}\)?
            \[
                \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1}
                \begin{bmatrix} T\alpha_1 & T\alpha_2 \end{bmatrix}
                =
                \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
                \begin{bmatrix} 1 & -i \\ 0 & 0 \end{bmatrix}
                =
                \begin{bmatrix} 1 & -i \\ 0 & 0 \end{bmatrix}
            \]
        \item[(c)] What is the matrix of \(T\) in the ordered basis \(\mathcal{B}'\)?
            \[
                \begin{bmatrix} 1 & -i \\ i & 2 \end{bmatrix}^{-1}
                \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
                \begin{bmatrix} 1 & -i \\ i & 2 \end{bmatrix}
                =
                \begin{bmatrix} 2 & i \\ -i & 1 \end{bmatrix}
                \begin{bmatrix} 1 & -i \\ 0 & 0 \end{bmatrix}
                =
                \begin{bmatrix} 2 & -2i \\-i & -i \end{bmatrix}
            \]
        \item[(d)] What is the matrix of \(T\) in the ordered basis \(\{\alpha_2, \alpha_1\}\)?
            \[
                \begin{bmatrix} -i & 1 \\ 2 & i \end{bmatrix}^{-1}
                \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
                \begin{bmatrix} -i & 1 \\ 2 & i \end{bmatrix}
                =
                \begin{bmatrix} -i & 1 \\ 2 & i \end{bmatrix}
                \begin{bmatrix} -i & 1 \\ 0 & 0 \end{bmatrix}
                =
                \begin{bmatrix} -1 & -i \\ -2i & 2 \end{bmatrix}
            \]
    \end{enumerate}

\item[2.] Let \(T\) be the linear transformation from \(\mathbb{R}^3\) into \(\mathbb{R}^2\) defined by
    \[ T(x_1, x_2, x_3) = (x_1 + x_2, 2x_3 - x_1). \]
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] If \(\mathcal{B}\) is the standard ordered basis for \(\mathbb{R}^3\) and
            \(\mathcal{B}'\) is the standard ordered basis for \(\mathcal{R}^2\), what is the matrix of \(T\) relative to the pair \(\mathcal{B}\), \(\mathcal{B}'\)?
            \[ \begin{bmatrix} 1 & 1 & 0 \\ -1 & 0 & 2 \end{bmatrix} \]
        \item[(b)] If \(\mathcal{B} = \{\alpha_1, \alpha_2, \alpha_3\}\) and \(\mathcal{B}' = \{\beta_1, \beta_2\}\) where
            \[ \alpha_1 = (1, 0, -1),\, \alpha_2 = (1, 1, 1),\, \alpha_3 = (1, 0, 0),\, \beta_1 = (0, 1),\, \beta_2 = (1, 0) \]
            what is the matrix of \(T\) relative to the pair \(\mathcal{B}, \mathcal{B}'\)?
            \[
                \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}^{-1}
                \begin{bmatrix} T\alpha_1 & T\alpha_2 & T\alpha_3 \end{bmatrix}
                =
                \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
                \begin{bmatrix} 1 & 2 & 1 \\ -3 & 1 & 0 \end{bmatrix}
                =
                \begin{bmatrix} -3 & 1 & 0 \\ 1 & 2 & 1 \end{bmatrix}
            \]
    \end{enumerate}

\item[3.] Let \(T\) be a linear operator on \(F^n\), let \(A\) be the matrix of \(T\) in the standard ordered basis for \(F^n\),
    and let \(W\) be the subspace of \(F^n\) spanned by the column vectors of \(A\). What does \(W\) have to do with \(T\)?

\[ A = \begin{bmatrix} T\epsilon_1 & \dots T\epsilon_n \end{bmatrix} \]
The range of \(T\) is spanned by \(T\epsilon_1, \dots, T\epsilon_n\), so it is identical to the column space of \(A\).

\item[4.] Let \(V\) be a two-dimensional vector space over the field \(F\), and let \(\mathcal{B}\) be an ordered basis for \(V\).
    If \(T\) is a linear operator on \(V\) and
    \[ [T]_\mathcal{B} = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \]
    prove that \(T^2 - (a + d)T + (ad - bc)I = 0\).
    \begin{gather*}
        [T^2 - (a + d)T + (ad - bc)I = 0]_\mathcal{B} = \\
        [T]_\mathcal{B}^2 - (a + d)[T]_\mathcal{B} + (ad - bc)I = \\
        \begin{bmatrix} a^2 + bc & b(a + d) \\ c(a + d) & d^2 + bc \end{bmatrix} -
        \begin{bmatrix} a^2 + da & b(a + d) \\ c(a + d) & d^2 + ad \end{bmatrix} +
        \begin{bmatrix} ad - bc & 0 \\ 0 & ad - bc \end{bmatrix} = \\
        \begin{bmatrix} bc - ad & 0 \\ 0 & bc - ad \end{bmatrix} +
        \begin{bmatrix} ad - bc & 0 \\ 0 & ad - bc \end{bmatrix} = 0
    \end{gather*}

\item[5.] Let \(T\) be the linear operator on \(\mathbb{R}^3\), the matrix of which in the standard ordered basis is
    \[ A = \begin{bmatrix} 1 & 2 & 1 \\ 0 & 1 & 1 \\ -1 & 3 & 4 \end{bmatrix}. \]
    Find a basis for the range of \(T\) and a basis for the null space of \(T\).

The range is spanned by the column space of \(A\), so column reduce \(A\);
\[ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -1 & 5 & 0 \end{bmatrix} \]
\(\{(1, 0, -1), (0, 1, 5)\}\) is a basis for the range.

\(A\) can be row reduced into
\[ \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & 1 \\ 0 & 0 & 0 \end{bmatrix} \]
Therefore \{(1, -1, 1)\} is a basis for the null space.

\item[6.] Let \(T\) be the linear operator on \(\mathbb{R}^2\) defined by
    \[ T(x_1, x_2) = (-x_2, x_1). \]

    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] What is the matrix of \(T\) in the standard ordered basis for \(\mathbb{R}^2\)?
            \[ \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \]

        \item[(b)] What is the matrix of \(T\) in the ordered basis \(\mathcal{B} = \{\alpha_1, \alpha_2\}\),
            where \(\alpha_1 = (1, 2)\) and \(\alpha_2 = (1, -1)\)?
            \[
                \begin{bmatrix} 1 & 1 \\ 2 & -1 \end{bmatrix}^{-1}
                \begin{bmatrix} -2 & 1 \\ 1 & 1 \end{bmatrix}
                =
                \frac{1}{3}
                \begin{bmatrix} -1 & 2 \\ -5 & 1 \end{bmatrix}
            \]

        \item[(c)] Prove that for every real number \(c\) the operator \(T - cI\) is invertible.

            The matrix of \(T - cI\) relative to the standard ordered basis is
            \[ \begin{bmatrix} -c & -1 \\ 1 & -c \end{bmatrix}, \]
            Noting that \(1 + c^2 \neq 0\) for all real \(c\), this can be inverted as follows,
            \[
                \begin{bmatrix} -c & -1 \\ 1 & -c \end{bmatrix} \rightarrow
                \begin{bmatrix} 1 & -c \\ -c & -1 \end{bmatrix} \rightarrow
                \begin{bmatrix} 1 & -c \\ 0 & -(1 + c^2) \end{bmatrix} \rightarrow
                \begin{bmatrix} 1 & -c \\ 0 & 1 \end{bmatrix} \rightarrow
                \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
            \]
            Therefore \(T - cI\) is invertible.

        \item[(d)] Prove that if \(\mathcal{B}\) is any ordered basis for \(\mathbb{R}^2\)
            and \([T]_\mathcal{B} = A\), then \(A_{12}A_{21} \neq 0\).

            Let \(\mathcal{B} = \{\alpha_1, \alpha_2\}\) where \(\alpha_1 = (x_1, x_2)\) and \(\alpha_2 = (y_1, y_2)\).
            \begin{gather*}
                [T]_\mathcal{B} =
                \begin{bmatrix} x_1 & y_1 \\ x_2 & y_2 \end{bmatrix}^{-1}
                \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
                \begin{bmatrix} x_1 & y_1 \\ x_2 & y_2 \end{bmatrix}
                = \\
                \begin{bmatrix} x_1 & y_1 \\ x_2 & y_2 \end{bmatrix}^{-1}
                \begin{bmatrix} -x_2 & -y_2 \\ x_1 & y_1 \end{bmatrix}
            \end{gather*}

            Find the inverse and multiply it to see \(A_{12}A_{21} \neq 0\) I'm not doing this
    \end{enumerate}

\item[8.] Let \(\theta\) be a real number. Prove that the following two matrices are similar over the field of complex numbers:
    \[
        \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}, \quad
        \begin{bmatrix} e^{i\theta} & 0 \\ 0 & e^{-i\theta} \end{bmatrix}
    \]
    (\textit{Hint:} Let \(T\) be the linear operator on \(\mathbb{C}^2\) which is represented by the first matrix in the standard ordered basis.
    Then find vectors \(\alpha_1\) and \(\alpha_2\) such that \(T\alpha_1 = e^{i\theta}\alpha_1\) and \(T\alpha_2 = e^{-i\theta}\alpha_2\)
    and \(\{\alpha_1, \alpha_2\}\) is a basis.)

    If \[ \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix} = [T]_{\mathcal{B}} \]
    then
    \begin{gather*}
        T(a_1, a_2) \\
        = (a_1\cos\theta - a_2\sin\theta, a_1\sin\theta + a_2\cos\theta) \\
        = (a_1\cos\theta + a_1i\sin\theta, a_2i\sin\theta + a_2\cos\theta) \\
        = (a_1, a_2)(\cos\theta + i\sin\theta) = e^{i\theta}(a_1, a_2)
    \end{gather*}
    \(a_1 = ia_2\) so \(\alpha_1 = (i, 1)\) is a vector such that \(T\alpha_1 = e^{i\theta}\alpha_1\).
    Similarly,
    \begin{gather*}
        T(b_1, b_2) \\
        = (b_1\cos\theta - b_2\sin\theta, b_1\sin\theta + b_2\cos\theta) \\
        = (b_1\cos\theta - b_1i\sin\theta, -b_2i\sin\theta + b_2\cos\theta) \\
        = (b_1, b_2)(\cos\theta - i\sin\theta) = e^{-i\theta}(b_1, b_2)
    \end{gather*}
    implies \(b_1 = -b_2i\) and \(\alpha_2 = (-i, 1)\) is a vector such that \(T\alpha_1 = e^{i\theta}\alpha_1\).
    The two are clearly linearly independent, so \(\{\alpha_1, \alpha_2\}\) form a basis, \(\mathcal{B'}\).

    \[ \begin{bmatrix} e^{i\theta} & 0 \\ 0 & e^{-i\theta} \end{bmatrix} = \begin{bmatrix}T\alpha_1 \\ T\alpha_2\end{bmatrix} = [T]_{\mathcal{B}'} \\ \]
    and by Theorem 14,
    \[ [T]_{\mathcal{B}}' = \begin{bmatrix}\alpha_1 & \alpha_2\end{bmatrix}^{-1} [T]_{\mathcal{B}} \begin{bmatrix}\alpha_1 & \alpha_2\end{bmatrix} \]
    and the two matrices are similar.

\item[9.] Let \(V\) be a finite-dimensional vector space over the field \(F\) and let \(S\) and \(T\) be linear operators on \(V\).
    We ask: When do there exist ordered basis \(\mathcal{B}\) and \(\mathcal{B}'\) for \(V\) such that \([S]_\mathcal{B} = [T]_\mathcal{B}'\)?
    Prove that such basis exist if and only if there an invertible operator \(U\) on \(V\) such that \(T = USU^{-1}\).
    (\textit{Outline of proof:} If \([S]_\mathcal{B} = [T]_\mathcal{B}'\),
    let \(U\) be the operator which carries \(\mathcal{B}\) onto \(\mathcal{B}'\) and show that \(S = UTU^{-1}\).
    Conversely, if \(T = USU^{-1}\) for some invertible \(U\), let \(\mathcal{B}\) be any basis for \(V\) and let \(\mathcal{B}'\) its image under \(U\).
    Then show that \([S]_\mathcal{B} = [T]_\mathcal{B'}\).)

    Let \([S]_\mathcal{B} = [T]_\mathcal{B'}\) and \(U\) be an invertible operator carrying \(\mathcal{B}\) onto \(\mathcal{B}'\).
    If \(\mathcal{B'} = \{\alpha_1, \dots, \alpha_n\}\), then the columns of matrix of \(U\) are \([\alpha_1]_\mathcal{B}, \dots, [\alpha_n]_\mathcal{B}\).

    By Theorem 14,
    \begin{gather*}
        [USU^{-1}]_\mathcal{B'} = [U^{-1}]_\mathcal{B}[USU^{-1}]_\mathcal{B}[U]_\mathcal{B} \\
        = [U^{-1}]_\mathcal{B}[U]_\mathcal{B}[S]_\mathcal{B}[U^{-1}]_\mathcal{B}[U]_\mathcal{B} \\
        = [S]_\mathcal{B} = [T]_\mathcal{B'}
    \end{gather*}
    and \(USU^{-1} = T\).

    If \(T = USU^{-1}\) for some invertible \(U\), let \(\mathcal{B}\) be any basis for \(V\) and let \(\mathcal{B'}\) its image under \(U\).
    \begin{gather*}
        [T]_\mathcal{B} = [USU^{-1}]_\mathcal{B'} = [U]_\mathcal{B}[S]_\mathcal{B}[U^{-1}]_\mathcal{B} \\
    \end{gather*}
    Then by Theorem 14, this is equivalent to \([U]_\mathcal{B'}\) and it must be that \([T]_\mathcal{B} = [U]_\mathcal{B'}\).

\item[10.] We have seen that the linear operator \(T\) on \(\mathbb{R}^2\) defined by \(T(x_1, x_2) = (x_1, 0)\)
    is represented in the standard ordered basis by the matrix
    \[ A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}. \]
    This operator satisfies \(T^2 = T\). Prove that if \(S\) is a linear operator on \(\mathbb{R}^2\) such that \(S^2 = S\),
    then \(S = 0\), or \(S = I\), or there is an ordered basis \(\mathcal{B}\) for \(\mathbb{R}^2\) such that \([S]_\mathcal{B} = A\).

    Suppose that \(S\epsilon_1\) and \(S\epsilon_2\) are linearly independent.
    Because \(S\) is an operator on \(\mathbb{R}^2\) to itself, \(S\) must be invertible.
    Then
    \begin{gather*}
        S^2 = S \\
        S^{-1}S^2 = S^{-1}S \\
        S = I
    \end{gather*}

    If not, \(S\epsilon_1\) and \(S\epsilon_2\) are linearly dependent. Then \(\rank S < 2\) and therefore \(\nullity S > 0\).

    If \(S\epsilon_1 = S\epsilon_2 = 0\), then \(S = 0\) (If \(v = a\epsilon_1 + b\epsilon_2\), \(Sv = aS\epsilon_1 + bS\epsilon\)), so WLOG assume \(S\epsilon_1 \neq 0\).
    Let \(v\) be a nonzero vector in the null space of \(S\).
    \(\mathcal{B} = \{S\epsilon_1, v\}\) is an ordered basis of \(\mathbb{R}^2\), as the two vectors are linearly independent.
    If not, there exists a real number \(a\) such that \(S\epsilon_1 = av\), which is a contradiction as because
    \(S\epsilon_1 = S(S\epsilon_1) = aSv = 0\). Then
    \begin{gather*}
        [S(S\epsilon_1)]_\mathcal{B} = [S\epsilon_1]_\mathcal{B} = \begin{bmatrix} 1 \\ 0 \end{bmatrix},\quad
        [Sv]_\mathcal{B} = [0]_\mathcal{B} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\
        [S]_\mathcal{B} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
    \end{gather*}
\end{enumerate}

\subsection{Linear Functionals}
\begin{enumerate}[listparindent=\parindent]

\item[1.] In \(\mathbb{R}^3\), let \(\alpha_1 = (1, 0, 1),\, \alpha_2 = (0, 1, -2),\, \alpha_3 = (-1, -1, 0)\).
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] If \(f\) is a linear functional on \(\mathbb{R}^3\) such that
            \[ f(\alpha_1) = 1,\quad f(\alpha_2) = -1,\quad f(\alpha_3) = 3, \]
            and if \(\alpha = (a, b, c)\), find \(f(\alpha)\).

            The coordinates of \(\alpha\) relative to the basis \(\{\alpha_1, \alpha_2, \alpha_3\}\) is
            \[
                \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 1 & -2 & 0\end{bmatrix}^{-1} \begin{bmatrix} a \\ b \\ c \end{bmatrix}
                = \begin{bmatrix} 2a - 2b - c \\ a - b - c \\ a - 2b - c \end{bmatrix}
            \]

            Then \(\alpha = (2a - 2b - c)\alpha_1 + (a - b - c)\alpha_2 + (a - 2b - c)\alpha_3\) and
            \begin{gather*}
                f(\alpha) = f((2a - 2b - c)\alpha_1 + (a - b - c)\alpha_2 + (a - 2b - c)\alpha_3) \\
                = (2a - 2b - c)f(\alpha_1) - (a - b - c)f(\alpha_2) + (a - 2b - c)f(\alpha_3) \\
                = (2a - 2b - c) - (a - b - c) + (a - 2b - c)3 \\
                = 4a - 7b - 3c
            \end{gather*}
        \item[(b)] Describe explicitly a linear functional \(f\) on \(\mathbb{R}^3\) such that
            \[ f(\alpha_1) = f(\alpha_2) = 0 \text{ but } f(\alpha_3) \neq 0. \]
            \[ f(x, y, z) = (-x + 2y + z)\]
        \item[(c)] Let \(f\) be any linear functional such that
            \[ f(\alpha_1) = f(\alpha_2) = 0 \text{ but } f(\alpha_3) \neq 0. \]
            If \(\alpha = (2, 3, -1)\), show that \(f(\alpha) \neq 0\).

            Use work from part a to find the coordinates of \(\alpha\) are \((-1, 0, -3)\).
            \begin{gather*}
                \alpha = -\alpha_1 - 3\alpha_3 \\
                f(\alpha) = -f(\alpha_1) - 3f(\alpha_3) \\
                f(\alpha) = -3f(\alpha_3) \neq 0
            \end{gather*}
    \end{enumerate}

\item[2.] Let \(\mathcal{B} = \{\alpha_1, \alpha_2, \alpha_3\}\) be the basis for \(\mathbb{C}^3\) defined by
    \[ \alpha_1 = (1, 0, -1),\quad \alpha_2 = (1, 1, 1),\quad \alpha_3 = (2, 2, 0). \]
    Find the dual basis of \(\mathcal{B}\).

    Find the solutions to the systems
    \begin{gather*}
        \begin{system}{3}
            x && &-& z &=& 1 \\
            x &+& y &+& z &=& 0 \\
            2x &+& 2y & & &=& 0 \\
        \end{system}
        \begin{system}{3}
            x && &-& z &=& 0 \\
            x &+& y &+& z &=& 1 \\
            2x &+& 2y & & &=& 0 \\
        \end{system}
        \begin{system}{3}
            x && &-& z &=& 0 \\
            x &+& y &+& z &=& 0 \\
            2x &+& 2y & & &=& 1 \\
        \end{system}
    \end{gather*}
    to get the dual basis \(\{f_1, f_2, f_3\}\), which turns out to be
    \begin{gather*}
        f_1(x, y, z) = x - z \\
        f_2(x, y, z) = x - y + 2z \\
        f_3(x, y, z) = -\frac{1}{2}x + y - \frac{1}{2}z
    \end{gather*}

\item[3.] If \(A\) and \(B\) are \(n \times n\) matrices over the field \(F\), show that \(\tr(AB) = \tr(BA)\).
    Now show that similar matrices have the same trace.
    \begin{align*}
        \tr(AB) &= \sum_{i = 1}^{n} (AB)_{ii} \\
                &= \sum_{i = 1}^{n} \sum_{j = 1}^{n} A_{ij}B_{ji} \\
                &= \sum_{j = 1}^{n} \sum_{i = 1}^{n} B_{ji}A_{ij} \\
                &= \sum_{j = 1}^{n} (BA)_{jj} \\
                &= \tr(BA)
    \end{align*}

    If \(A = P^{-1}BP\),
    \begin{gather*}
        \tr(A) = \tr(P^{-1}BP) = \tr(P^{-1}PB) = \tr(IB) = \tr(B)
    \end{gather*}

\item[4.] Let \(V\) be the vector space of all polynomial functions \(p\) from \(\mathbb{R}\) into \(\mathbb{R}\)
    which have degree 2 or less: \[ p(x) = c_0 + c_1x + c_2x^2. \]
    Define three linear functionals on \(V\) by
    \[ f_1(p) = \int_0^1 p(x)\, dx,\quad f_2(p) = \int_0^2 p(x)\, dx,\quad f_3(p) = \int_0^{-1} p(x)\, dx \]
    Show that \(\{f_1, f_2, f_3\}\) is a basis for \(V^*\) by exhibiting the basis for \(V\) of which it is the dual.
    \begin{align*}
        \int p(x)\, dx &= c_0x + \frac{c_1}{2}x^2 + \frac{c_3}{3}x^3 \\
        f_1(p) &= c_0 + \frac{c_1}{2} + \frac{c_3}{3} \\
        f_2(p) &= 2c_0 + 2c_1 + \frac{8c_3}{3} \\
        f_3(p) &= -c_0 + \frac{c_1}{2} - \frac{c_3}{3}
    \end{align*}
    Then each basis \((c_0, c_1, c_2)\) satisfies
    \begin{gather*}
        \begin{bmatrix}
            1 & \frac{1}{2} & \frac{1}{3} \\
            2 & 2 & \frac{8}{3} \\
            -1 & \frac{1}{2} & \frac{-1}{3}
        \end{bmatrix}
        \begin{bmatrix} c_0 \\ c_1 \\ c_2 \end{bmatrix} =
        \begin{bmatrix} x \\ y \\ z \end{bmatrix}
    \end{gather*}
    where \((x, y, z)\) is \((1, 0, 0)\), \((0, 1, 0)\), or \((0, 0, 1)\).
    Skipping the fun part of inverting the coefficient matrix and setting \((x, y, z)\) to be each of the three choices,
    the basis \(\{v_1, v_2, v_3\}\) that is dual to \(\{f_1, f_2, f_3\}\) is
    \begin{align*}
        v_1 &= 1 + x + \frac{-3}{2}x^2 \\
        v_2 &= \frac{-1}{6} +  \frac{1}{2}x^2 \\
        v_3 &= \frac{-1}{3} + x + \frac{-1}{2}x^2 \\
    \end{align*}

\item[5.] If \(A\) and \(B\) are \(n \times n\) complex matrices, show that \(AB - BA = I\) is impossible.

    \(\tr(I) = 1\), and Exercise 3 proved \(\tr(AB) = \tr(BA)\). We also know that trace is a linear function.
    If \(AB - BA = I\), then
    \begin{gather*}
        \tr(AB - BA) = \tr(I) \\
        \tr(AB) - \tr(BA) = \tr(I) \\
        0 = 1
    \end{gather*}

\item[6.] Let \(m\) and \(n\) be positive integers and \(F\) a field.
    Let \(f_1, \dots, f_n\) be linear functionals on \(F^m\). For \(\alpha\) in \(F^n\) define
    \[ T\alpha = (f_1(\alpha), \dots, f_m(\alpha)) \]
    Show that \(T\) is a linear transformation from \(F^n\) into \(F^m\).
    Then show that every linear transformation from \(F^n\) into \(F^m\) is of the above form, for some \(f_1, \dots, f_n\).

    Let \(a, b \in F^n\) and \(c \in F\). By linearity of linear functionals,
    \begin{align*}
        T(ca + b) &= (f_1(ca + b), \dots) \\
                  &= (cf_1(a) + f_1(b)), \dots) \\
                  &= c(f_1(a) + \dots) + (f_1(b), \dots) \\
                  &= cT(a) + T(b)
    \end{align*}
    so \(T\) must be a linear transformation from \(F^n\) into \(F^m\).

    Let \(U\) be a linear transformation from \(F^n\) into \(F^m\).
    Let \(v_1, \dots, v_n\) be a basis of \(F^n\), and \(w_1, \dots, w_n\) be its image under \(U\).
    For any \(\alpha \in F^n\),
    \begin{align*}
        \alpha &= \alpha_1v_1 + \dots + \alpha_nv_n \\
        U\alpha &= \alpha_1w_1 + \dots + \alpha_nw_n \\
        U\alpha &= (\alpha_1(w_1)_1 + \dots + \alpha_n(w_n)_n,\, \dots,\, \alpha_n(w_n)_m + \dots + \alpha_n(w_n)_m)
    \end{align*}
    where \((w)_i\) denotes the \(i\)th element of the \(m\)-tuple \(w\).

    Therefore, if we define linear functionals \(f_1, \dots, f_m\) by letting
    \[f_i(\alpha) = \sum_{j=0}^n \alpha_j(w_j)_i\]
    then
    \[U\alpha = (f_1(\alpha), \dots, f_m(\alpha))\]

\item[7.] Let \(\alpha_1 = (1, 0, -1, 2)\), and \(\alpha_2 = (2, 3, 1, 1)\),
    and let \(W\) be the subspace of \(\mathbb{R}^4\) spanned by \(\alpha_1\) and \(\alpha_2\).
    Which linear functionals \(f\):
    \[ f(x_1 + x_2 + x_3 + x_4) = c_1x_1 + c_2x_2 + c_3x_3 + c_4x_4 \]
    are in the annihilator of \(W\)?
    \begin{gather*}
        \begin{system}{4}
            c_1 &-& && c_3 &+& 2c_4 &=& 0 \\
            2c_1 &+& 3c_2 &+& c_3 &+& c_4 &=& 0 \\
        \end{system} \\
        \begin{system}{4}
            c_1 &-& && c_3 &+& 2c_4 &=& 0 \\
            && c_2 &+& c_3 &-& c_4 &=& 0 \\
        \end{system}
    \end{gather*} \(W\) is annihilated by the set of all \(f\) where \[ f(x_1, x_2, x_3, x_4) = (a - 2b)x_1 + (-a + b)x_2 + ax_3 + bx_4,\quad a, b \in \mathbb{R} \]

\item[8.] Let \(W\) be the subspace of \(\mathbb{R}^5\) which is spanned by the vectors
    \begin{align*}
        \alpha_1 &= \epsilon_1 + 2\epsilon_2 + \epsilon_3 \\
        \alpha_2 &= \epsilon_2 + 3\epsilon_3 + 3\epsilon_4 + \epsilon_5 \\
        \alpha_3 &= \epsilon_1 + 4\epsilon_2 + 6\epsilon_3 + 4\epsilon_4 + \epsilon_5
    \end{align*}
    Find a basis for \(W^0\).
    \begin{gather*}
        \begin{bmatrix}
            1 & 2 & 1 & 0 & 0 \\
            0 & 1 & 3 & 3 & 1 \\
            1 & 4 & 6 & 4 & 1
        \end{bmatrix}
        \begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \end{bmatrix}
        = 0 \\
        \\
        \begin{bmatrix}
            1 & 2 & 1 & 0 & 0 \\
            0 & 1 & 3 & 3 & 1 \\
            1 & 4 & 6 & 4 & 1
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 0 & 4 & 3 \\
            0 & 1 & 0 & -3 & -2 \\
            0 & 0 & 1 & 2 & 1
        \end{bmatrix}
    \end{gather*}

    \[ f(x_1, x_2, x_3, x_4, x_5) = (-4a - 3b)x_1 + (3a + 2b)x_2 + (-2a - b)x_3 + ax_4 + bx_5 \]

    A basis of \(W^0\) is \(\{f_0, f_1\}\) where
    \begin{align*}
        f_0(x_1, x_2, x_3, x_4, x_5) &= -4x_1 + 3x_2 - 2x_3 + x_4 \\
        f_1(x_1, x_2, x_3, x_4, x_5) &= -3x_1 + 2x_2 - x_3 + x_5
    \end{align*}


\item[9.] Let \(V\) be the vector space of all \(2 \times 2\) matrices over the field of real numbers and let
    \[ B = \begin{bmatrix} 2 & -2 \\ -1 & 1 \end{bmatrix} \]
    Let \(W\) be the subspace of \(V\) consisting of all \(A\) such that \(AB = 0\).
    Let \(f\) be a linear functional on \(V\) which is in the annihilator of \(W\).
    Suppose that \(f(I) = 0\) and \(f(C) = 3\), where \(I\) is the \(2 \times 2\) identity matrix and
    \[ C = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \]
    Find \(f(B)\).
    \begin{gather*}
        0 = AB = A\begin{bmatrix} 2 & -2 \\ -1 & 1 \end{bmatrix} \\
        A = \begin{bmatrix} x & 2x \\ y & 2y \end{bmatrix},\, x, y \in \mathbb{R}
    \end{gather*}

    Let \(f(A) = c_1A_{11} + c_2A_{12} + c_3A_{21} + c_4A_{22}\) and \(f(A) = 0\) then
    \[ 0 = c_1x + 2c_2x + c_2y+ 2c_3y = (c_1 + 2c_2)x + (c_3 + 2c_4)y \]
    implies \(c_1 = -2c_2\) and \(c_3 = -2c_4\), so all \(f\) in \(W^0\) is in the form
    \[ f(A) = -2c_2A_{11} + c_2A_{12} - 2c_4A_{21} + c_4A_{22} \]

    If \(f(C) = 3\), then \(c_4 = 3\).
    Likewise, \(f(I) = 0\) implies \(0 = -2c_2 + c_4\) and \(c_2 = \frac{3}{2}\).
    Then \(f(A) = -3A_{11} + \frac{3}{2}A_{12} - 6A_{21} + 3A_{22}\) and \(f(B) = 0\).

\item[10.] Let \(F\) be a subfield of the complex numbers. We define \(n\) linear functionals on \(F^n (n \geq 2)\) by
    \[ f_k(x_1, \dots, x_n) = \sum_{j=1}^n (k - j)x_j ,\quad 1 \leq k \leq n. \]
    What is the dimension of the subspace annihilated by \(f_1, \dots, f_n\)?

    The system of equations
    \[
        \begin{system}{1}
            f_1(v) &= 0 \\
            \vdots \\
            f_n(v) &= 0 \\
        \end{system}
    \]
    is represented by the matrix
    \[
        \begin{bmatrix}
            0 & -1 & -2 & \dots & 1 - n \\
            1 & 0 & -1 & \dots & 2 - n \\
            2 & 1 & 0 & \dots & 3 - n \\
            3 & 2 & 1 & \dots & 3 - n \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
        \end{bmatrix}
    \]
    Row reduce by adding the first row to all the other rows.
    \begin{gather*}
        \begin{bmatrix}
            0 & -1 & -2 & \dots & 1 - n \\
            1 & 1 & 1 & \dots & 1 \\
            2 & 2 & 2 & \dots & 2 \\
            3 & 3 & 3 & \dots & 3 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            0 & -1 & -2 & \dots & 1 - n \\
            1 & 1 & 1 & \dots & 1 \\
            0 & 0 & 0 & \dots & 0 \\
            0 & 0 & 0 & \dots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
        \end{bmatrix}
    \end{gather*}
    The null space of this matrix, and therefore the subspace annihilated by \(f_1, \dots, f_n\), has dimension \(n - 2\).

\item[11.] Let \(W_1\) and \(W_2\) be subspaces of a finite-dimensional vector space \(V\).
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Prove that \((W_1 + W_2)^0 = W_1^0 \cap W_2^0\).

            Suppose \(f \in (W_1 + W_2)^0\).
            Because \(W_1 \subseteq (W_1 + W_2)\), \(f(v) = 0\) for any \(f\) in \(W_1^0\) and \(v\) in \(W_1\).
            Similarly, it must be that \(f \in W_2^0\).
            Therefore \((W_1 + W_2)^0 \subseteq W_1^0 \cap W_2^0\).

            Next, suppose \(f \in W_1^0 \cap W_2^0\).
            Then if \(v \in W_1\) and \(w \in W_2\), then \(f(v) = f(w) = 0\), and \(f(v + w) = f(v) + f(w) = 0\).
            Since \(v + w \in W_1 + W_2\), it must be the case that \(f \in (W_1 + W_2)^0\).
            So \(W_1^0 \cap W_2^0 \subseteq (W_1 + W_2)^0\). 

            Since the two sets are subsets of each other, they are identical.

        \item[(b)] Prove that \((W_1 \cap W_2)^0 = W_1^0 + W_2^0\).

            Let \(f \in W_1^0\) and \(g \in W_2^0\).
            If \(v \in W_1 \cap W_2\), \(f(v) = g(v) = 0\), so \(f + g = 0\).
            Therefore \(W_1^0 + W_2^0 \subseteq (W_1 \cap W_2)^0\).

            How do I prove the other part though, :thonk:

    \end{enumerate}

\item[12.] Let \(V\) be a finite-dimensional vector space over the field \(F\) and let \(W\) be a subspace of \(V\).
    If \(f\) is a linear functional on \(W\), prove that there is a linear functional \(g\) on \(V\) such that \(g(\alpha) = f(\alpha)\) for each \(\alpha\) in \(W\).

    Let \(\{v_1, \dots, v_n\}\) a basis of \(W\), and \(\{f_1, \dots, f_n\}\) its dual basis in \(W^*\).
    Then if \(f\) is a linear functional on \(W\), it is of the form
    \[ f = \sum_{i=1}^n c_if_i,\quad c_i \in F \]

    Extend \(\{v_1, \dots, v_n\}\) into \(\{v_1, \dots, v_m\}\) to form a basis of \(V\).
    Its dual basis can also be extended into a dual basis of \(V^*\), \(\{f_1, \dots, f_m\}\). Let
    \[ g = \sum_{i=1}^m c_if_i \]

    If \(\alpha \in W\), then \(f_i(\alpha) = 0\) for all \(i > n\).
    By the definition of a dual basis, \(f_i(v_j) = \delta_{ij}\), and \(\alpha = c_1v_1 + \dots + c_nv_n + 0v_{n+1} + \dots + 0v_m\).
    \[ g(\alpha) = \sum_{i=1}^m c_if_i = \sum_{i=1}^n c_if_i = f(\alpha) \]

\item[13.] Let \(F\) be a subfield of the field of complex numbers and let \(V\) be any vector space over \(F\).
    Suppose that \(f\) and \(g\) are linear functionals on \(V\) such that the function \(h\) defined by \(h(\alpha) = f(\alpha)g(\alpha)\) is also a linear functional on \(V\).
    Prove that either \(f = 0\) or \(g = 0\).

    Let \(v \in V\) and \(c \in F\), \(c \neq 0\). Since \(h\) is linear, \(h(cv) = ch(v)\), but
    \begin{align*}
        h(cv) &= f(cv)g(cv) \\
              &= c^2f(v)g(v) \\
              &= c^2h(v)
    \end{align*}
    and therefore \(h = 0\).

    If \(v, w \in V\), then
    \begin{align*}
        h(v+w) &= f(v+w)g(v+v) \\
               &= (f(v) + f(w))(g(v) + g(w)) \\
               &= f(v)g(v) + f(v)g(w) + f(w)g(v) + f(w)g(w) \\
               &= h(v) + h(w) + f(v)g(w) + f(w)g(v) \\
               &= f(v)g(w) + f(w)g(v)
    \end{align*}
    \(h = 0\), so it must be the case that \(f(v)g(w) + f(w)g(v) = 0\).

    We are done if both are 0; otherwise, WLOG suppose \(f\) is a nonzero function.
    Then there exists some \(\alpha \in V\) such that \(f(\alpha) \neq 0\). Then \(g(\alpha) = 0\) because \(f(\alpha)g(\alpha) = 0\).
    Then for any \(\beta \in V\),
    \begin{align*}
        f(\alpha)g(\beta) + f(\beta)g(\alpha) &= 0 \\
        f(\alpha)g(\beta) + f(\beta)0 &= 0 \\
        f(\alpha)g(\beta) &= 0
    \end{align*}
    and since \(f(\alpha) \neq 0\), \(g(\beta) = 0\) and \(g = 0\).

\end{enumerate}

\subsection{The Double Dual}
\begin{enumerate}[listparindent=\parindent]
\item[1.] Let \(n\) be a positive integer and \(F\) a field.
    Let \(W\) be the set of all vectors \((x_1, \dots, x_n)\) in \(F^n\) such that \(x_1 + \dots + x_n = 0\).
\begin{enumerate}[listparindent=\parindent]
    \item[(a)] Prove that \(W^0\) consists of all linear functionals \(f\) of the form
        \[ f(x_1, \dots, x_n) = c\sum_{j=1}^n x_j. \]

        Let \(f \in W^0\).
        \begin{align*}
            f(x_1, \dots, x_n) &= c_1x_1 + \dots + c_nx_n \\
            f(1, -1, 0, 0, \dots, 0) &= c_1 - c_2 = 0 \\
            f(1, 0, -1, 0, \dots, 0) &= c_1 - c_3 = 0 \\
            \vdots \\
            f(1, 0, 0, \dots, 0, -1) &= c_1 - c_n = 0
        \end{align*}
        which implies \(c_1 = \dots = c_n = c\) and
        \[ f(x_1, \dots, x_n) = c\sum_{j=1}^n x_j. \]

        Conversely, if \(f\) is a functional in this form, then \(\sum_{j=1}^n x_j = 0\) by the definition of \(W\), so \(f \in W^0\).

    \item[(b)] Show that the dual space \(W^*\) of \(W\) can be \lq{naturally}\rq identified with the linear functionals
        \[ f(x_1, \dots, x_n)= c_1x_1 + \dots + c_nx_n \]
        on \(F^n\) which satisfy \(c_1 + \dots + c_n = 0\).

        Any vector in \(W\), \((c_1, c_2, \dots, c_n)\), naturally corresponds to the linear functional
        \[ f(x_1, \dots, x_n) = c_1x_1, + c_2x_2, \dots, c_nx_n \]
\end{enumerate}

\item[2.] Use Theorem 20 to prove the following.
    If \(W\) is a subspace of a finite-dimensional vector space \(V\) and if \(\{g_1, \dots, g_r\}\) is any basis for \(W^0\), then
    \[ W = \bigcap_{i=1}^r N_{g_i}. \]
    
    TODO: I have no idea how to do this

\item[3.] Let \(S\) be a set, \(F\) a field, and \(V(S; F)\) the space of all functions from \(S\) into \(F\):
    \begin{align*}
        (f + g)(x) &= f(x) + g(x) \\
           (cf)(x) &= cf(x).
    \end{align*}
    Let \(W\) be any \(n\)-dimensional subspace of \(V(S; F)\).
    Show that there exist points \(x_1, \dots, x_n\) in \(S\) and functions \(f_1, \dots, f_n\) in \(W\) such that \(f_i(x_j) = \delta_{ij}\).

    TODO: I have no idea how to do this

\end{enumerate}

\subsection{The Transpose of a Linear Transformation}
\begin{enumerate}[listparindent=\parindent]
\item[1.] Let \(F\) be a field and let \(f\) be the linear functional on \(F^2\) defined by
    \(f(x_1, x_2) = ax_1 + bx_2\). For each of the following linear operators \(T\), let \(g = T^tf\),
    and find \(g(x_1, x_2)\).

    \item[(a)] \(T(x_1, x_2) = (x_1, 0);\)
    \[ g(x_1, x_2) = T^tf(x_1, x_2) = f(T(x_1, x_2)) = f(x_1, 0) = ax_1 \]
    \item[(b)] \(T(x_1, x_2) = (-x_2, x_1);\)
    \[ g(x_1, x_2) = f(-x_2, x_1) = -ax_2 + bx_1 \]
    \item[(c)] \(T(x_1, x_2) = (x_1 - x_2, x_1 + x_2);\)
    \[ g(x_1, x_2) = f(x_1 - x_2, x_1 + x_2) = a(x_1 - x_2) + b(x_1 + x_2) \]

\item[2.] Let \(V\) be the vector space of all polynomial functions over the field of real numbers.
    Let \(a\) and \(b\) be fixed real numbers and let \(f\) be the linear functional on \(V\) defined by
    \[ f(p) = \int^b_a p(x) \, dx. \]
    If \(D\) is the differentiation operator on \(V\), what is \(D^tf\)?
    \begin{align*}
        (D^tf)p &= f(D(p)) \\
                &= f(c_1 + 2c_2x + \dots + nc_nx^{n-1}) \\
                &= c_1x + c_2x^2 + \dots + c_nx^n \rvert^b_a \\
                &= p(b) - p(a)
    \end{align*}

\item[3.] Let \(V\) be the space of all \(n \times n\) matrices over a field \(F\)
    and let \(B\) be a fixed \(n \times n\) matrix. If \(T\) is the linear operator on \(V\) defined by \(T(A) = AB - BA\),
    and \(f\) is the trace function, what is \(T^tf\)?

    We previously proved \(\tr(AB) = \tr(BA)\), and \(\tr(AB - BA) = \tr(AB) - \tr(BA) = 0\) by linearity.
    Therefore \(T^tf\) is the zero linear functional.

\item[4.] Let \(V\) be a finite-dimensional vector space over the field \(F\) and let \(T\) be a linear operator on \(V\).
    Let \(c\) be a scalar and suppose there is a non-zero vector \(\alpha\) in \(V\) such that \(T\alpha = c\alpha\).
    Prove that there is a non-zero linear functional \(f\) on \(V\) such that \(T^tf = cf\).

    By Theorem 22, because \(V\) is a finite-dimensional space,
    any linear transformation on \(V\) and its transpose have the same rank, which implies they also have the same nullity.
    \((T - cI)\alpha = 0\), so \(\nullity (T - cI) = \nullity (T - cI)^t > 0\).
    It follows that there exists a non-zero linear functional \(f\) such that \((T - cI)^tf = (T^t - cI^t)f = 0\), and \(T^tf = cf\).

\item[5.] Let \(A\) be an \(m \times n\) matrix with \textit{real} entries. Prove that \(A = 0\) if and only if \(\tr(A^tA) = 0\).

    If \(A = 0\), clearly \(\tr(A^tA) = 0\).
    \begin{align*}
        \tr(A^tA) &= \sum_{i=1}^n (A^tA)_{ii} \\
                  &= \sum_{i=1}^n \sum_{j=1}^m A^t_{ij}A_{ji} \\
                  &= \sum_{i=1}^n \sum_{j=1}^m A_{ji}A_{ji} \\
                  &= \sum_{i=1}^n \sum_{j=1}^m A_{ji}^2
    \end{align*}
    which is the sum of square of every entry of \(A\).
    Since \(A\) has real entries, \(\tr(A^tA) = 0\) implies every entry is zero and \(A = 0\).

\item[6.] Let \(n\) be a positive integer and let \(V\) be the space of all polynomial functions over the field of real numbers
    which have degree at most \(n\), i.e., functions of the form
    \[ f(x) = c_0 + c_1x + \dots + c_nx^n. \]
    Let \(D\) be the differentiation operator on \(V\). Find a basis for the null space of the transpose operator \(D^t\).

    By Theorem 22, the null space of \(D^t\) is the annihilator of the range of \(D\).
    The range of \(D\) is all polynomials of the form
    \[ c_1 + c_2x + \dots + c_{n-1}x^{n-1} \]
    which has a dimension of \(n - 1\), so its annihilator has a dimension of \(n - (n - 1) = 1\).
    Let \(f(c_0 + c_1x + \dots + c_nx^n) = c_n\).
    Then \(\{f\}\) must a basis of the annihilator of the range of \(D\).
    Therefore a basis of the null space of \(D^t\).

\item[7.] Let \(V\) be a finite-dimensional vector space over the field \(F\).
    Show that \(T \rightarrow T^t\) is an isomorphism of \(L(V, V)\) onto \(L(V^*, V^*)\).

    Suppose \(T^t = 0\), and let the null space of \(T\) be \(N_T\).
    By Theorem 22, the annihilator of \(N_T\) is the range of \(T^t\), which is the zero subspace of \(V^*\).
    \begin{gather*}
        \dim N_T^0 + \dim N_T = \dim N \\
        0 + \dim N_T = \dim N \\
        \dim N_T = \dim N
    \end{gather*}
    This implies \(N_T = N\) because \(N_T \subseteq N\), and subsequently \(T = 0\) and transposition must be injective.
    We know \(V\) and \(V^*\) are isomorphic, so their dimensions are equal, and it follows that
    \(\dim L(V, V) = (\dim V)^2 = (\dim V^*)^2 = \dim L(V^*, V^*)\).
    \(V\) is a finite space, so any injective linear maps are also isomorphisms.

\item[8.] Let \(V\) be the vector space of \(n \times n\) matrices over the field \(F\).
\begin{enumerate}[listparindent=\parindent]
\item[(a)] If \(B\) is a fixed \(n \times n\) matrix, define a function \(f_B\) on \(V\) by \(f_B(A) = \tr(B^tA)\).
    Show that \(f_B\) is a linear functional on \(V\).
    \begin{align*}
        f_B(cA_1 + A_2) &= \tr(B^t(cA_1 + A_2)) \\
                        &= \tr(cB^t(A_1 + A_2)) \\
                        &= \tr(cB^tA_1 + B^tA_2)) \\
                        &= \tr(cB^tA_1) + \tr(B^tA_2) \\
                        &= c\tr(B^tA_1) + \tr(B^tA_2) \\
                        &= cf_B(A_1) + f_B(A_2)
    \end{align*}

\item[(b)] Show that every linear functional on \(V\) is of the above form, i.e., is \(f_B\) for some \(B\).

    Let \(f_B\) be a linear functional on \(V\);
    \[f_BA = \sum_{i=1}^{n} \sum_{j=1}^{n} c_{ji}A_{ji}\]
    Then there exists a matrix \(B\) such that \(f_B(A) = \tr(B^tA)\);
    \begin{align*}
        (B^tA)_{ii} &= \sum_{j=1}^{n} B_{ji}A_{ji} \\
        \tr(B^tA) &= \sum_{i=1}^{n} (B^tA)_{ii} \\
                  &= \sum_{i=1}^{n} \sum_{j=1}^{n} B_{ji}A_{ji} \\
                  &= f_BA
    \end{align*}

\item[(c)] Show that \(B \rightarrow f_B\) is an isomorphism of \(V\) onto \(V^*\).

    Let \(f: B \rightarrow f_B\). We'll show \(f\) is a linear function.
    Let \(c \in F\) and \(B_1, B_2 \in V\). Then
    \[\tr(cB_1 + B_2)^t = \tr(cB_1)^t + \tr B_2^t = c(\tr B_1)^t + \tr B_2^t\]
    by linearity of the trace function.
    
    Part B proved \(f\) surjective; for any \(g \in V^*\) there exists a \(B \in V\) such that \(fB = g\).
    We know that any vector space and its dual are isomorphic, so \(\dim V = \dim V^*\).
    \(V\) is finite, so surjection implies injection, and therefore \(f\) is an isomorphism.

\end{enumerate}

\end{enumerate}
\end{document}
