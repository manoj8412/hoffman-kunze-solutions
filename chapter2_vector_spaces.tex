\documentclass{article}
\usepackage{array}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{
    bookmarksnumbered=true,
    pdfpagemode=UseOutlines,
}

\input{util.tex}

\begin{document}

\setcounter{section}{1}
\section{Vector Spaces}

\subsection{Vector Spaces}
\begin{enumerate}[listparindent=\parindent]
\item[1.] If \(F\) is a field, verify that \(F^n\) (as defined in Example 1) is a vector space over the field \(F\).

Let \(\alpha = (x_1, x_2, \dots, x_n)\), \(\beta = (y_1, y_2, \dots, y_n)\), and \(\gamma = (z_1, z_2, \dots, z_n)\).
First we prove the properties of vector addition, using the definition of addition over a field:
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Addition is commutative.
        \begin{gather*}
            \alpha + \beta \\
            = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n) \\
            = (y_1 + x_1, y_2 + x_2, \dots, y_n + x_n) \\
            = \beta + \alpha
        \end{gather*}

        \item[(b)] Addition is associative.
        \begin{gather*}
            (\alpha + \beta) + \gamma \\
            = ((x_1 + y_1) + z_1, (x_2 + y_2) + z_2, \dots, (x_n + y_n) + z_n) \\
            = (x_1 + (y_1 + z_1), x_2 + (y_2 + z_2), \dots, x_n + (y_n + z_n)) \\
            = \alpha + (\beta + \gamma)
        \end{gather*}

        \item[(c)] There is a unique vector 0 such that \(\alpha + 0 = \alpha\) for all \(\alpha\) in V.

            Since \(F\) is a field, there exists a unique element 0 in \(F\) such that \(x + 0 = x\) for all \(x\) in \(F\),
            so there is a unique zero vector which is a \(n\)-tuple containing all 0s:
            \[(x_1 + 0, \dots, x_n + 0) = (x_1, \dots, x_n) + (0, \dots, 0) = \alpha + 0\]

        \item[(d)] For each vector \(\alpha\) in \(V\) there is a unique vector \(-\alpha\) in \(V\) such that \(\alpha + (-\alpha) = 0\).

            Since \(F\) is a field, there exists a unique element \(-x\) in \(F\) such that \(x + (-x) = x\) for all \(x\) in \(F\). Therefore,
            \begin{gather*}
                (x_1 + -(x_1), \dots, x_n + -(x_n)) \\
                = (x_1, \dots, x_n) + (-x_1, \dots, -x_n) \\
                = \alpha + -\alpha
            \end{gather*}
    \end{enumerate}

Then we prove the properties of scalar multiplication:
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] \(1\alpha = \alpha\) for every \(\alpha \in V\).
        \[1\alpha = (1x_1, \dots, 1x_n) = (x_1, \dots, x_n) = \alpha\]

        \item[(b)] \((c_1c_2)\alpha = c_1(c_2\alpha)\).
        \begin{gather*}
            (c_1c_2)\alpha = ((c_1c_2)x_1, \dots, (c_1c_2)x_n) \\
            = (c_1(c_2x_1), \dots, c_1(c_wx_n)) \\
            = c_1(c_2\alpha)
        \end{gather*}

        \item[(c)] \(c(\alpha + \beta) = c\alpha + c\beta\)
        \begin{gather*}
            c(\alpha + \beta) = c(x_1 + y_1, \dots, x_n + y_n) \\
            = (c(x_1 + y_1), \dots, c(x_n + y_n)) \\
            = (cx_1 + cy_1, \dots, cx_n + cy_n) \\
            = c\alpha + c\beta
        \end{gather*}

        \item[(d)] \((c_1 + c_2)\alpha = c_1\alpha + c_2\alpha\)
        \begin{gather*}
            (c_1 + c_2)\alpha = ((c_1 + c_2)x_1, \dots, (c_1 + c_2)x_n) \\
            = (c_1x_1 + c_2x_1, \dots, c_1x_n + c_2x_n) \\
            = c_1\alpha + c_2\alpha
        \end{gather*}
    \end{enumerate}

As it satisfies all the properties of a vector space, \(R^n\) is a vector space over \(F\).

\item[2.] If \(V\) is a vector space over the field \(F\), verify that
    \[(\alpha_1 + \alpha_2) + (\alpha_3 + \alpha_4) = [\alpha_2 + (\alpha_3 + \alpha_1)] + \alpha_4\]
    for all vectors \(\alpha_1, \alpha_2, \alpha_3, \alpha_4 \in V\).
    \begin{gather*}
        (\alpha_1 + \alpha_2) + (\alpha_3 + \alpha_4) \\
        = [(\alpha_2 + \alpha_1) + \alpha_3] + \alpha_4 \\
        = [\alpha_2 + (\alpha_1 + \alpha_3)] + \alpha_4 \\
        = [\alpha_2 + (\alpha_3 + \alpha_1)] + \alpha_4
    \end{gather*}

\item[3.] If \(C\) is the field of complex numbers, which vectors in \(C^3\) are linear combinations of
    \((1, 0, -1), (0, 1, 1)\) and \((1, 1, 1)\)?

Let \((y_1, y_2, y_3)\) be a vector formed by linear combinations of the three given vectors.
Then for some scalars \(x_1, x_2, x_3 \in C\),
\[
    \begin{bmatrix}
        1 & 0 & 1 \\
        0 & 1 & 1 \\
        -1 & 1 & 1 \\
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
        y_1 \\ y_2 \\ y_3
    \end{bmatrix}
\]
or in short, \(AX = Y\). Elementary row operations can show that \(A\) is row-equivalent to \(I\),
and therefore \(A\) is invertible. We can then solve the system; \(X = A^{-1}Y\).
This means any vector \(Y\) over \(C\) can be written as a linear combination of the three given vectors.

\item[4.] Let \(V\) be the set of all pairs \((x, y)\) of real numbers, and let \(F\) be the field of real numbers.
    Define

    \[(x, y) + (x_1, y_1) = (x + x_1, y + y_1)\]
    \[c(x, y) = (cx, y).\]
    Is \(V\), with these operations, a vector space over the field of real numbers?

\(V\) is not a vector space because there is no zero scalar \(0\) such that \(0(x, y) = 0\) for all \(y \in V\);
\(0(x, y) = (0x, y) \neq (0, 0)\) unless \(y = 0\).

\item[6.] Let \(V\) be the set of all complex-valued function \(f\) on the real line such that (for all \(t\) in \(\mathbb{R}\))
    \[f(-t) = \overline{f(t)}.\]
    The bar denotes complex conjugation. Show that \(V\), with operations
    \begin{gather*}
        (f + g)(t) = f(t) + g(t) \\
        (cf)(t) = cf(t)
    \end{gather*}
    is a vector space over the field of \textit{real} numbers. Give an example of a function in \(V\) which is not real-valued.

    First, for an example of a not real-valued function in \(V\), let \(f(t) = ti\). Then \(f(-t) = -ti = \overline{ti} = \overline{f(t)}\).

    \(V\) is closed under addition and multiplication. Suppose \(c \in \mathbb {R}\) and \(f, g \in V\). Then,
    \begin{gather*}
        (f + g)(-t) \\
        = f(-t) + g(-t) \\
        = \overline{f(t)} + \overline{g(t)} \\
        = \overline{f(t) + g(t)} \\
        = \overline{(f + g)(t)} \\
        (f + g)(t) \in V
    \end{gather*}
    \begin{gather*}
        (cf)(-t) \\
        = cf(-t) \\
        = c\overline{f(t)} \\
        = \overline{cf(t)}
        = \overline{(cf)(t)} \\
        (cf)(t) \in V
    \end{gather*}

    Commutativity and associativity directly follow from properties of addition and multiplication in \(\mathbb{C}\).
    \[(f + g)(t) = f(t) + g(t) = g(t) + f(t) = (g + f)(t)\]
    \[((f + g) + h)(t) = (f(t) + g(t)) + h(t) = \\ f(t) + (g(t) + h(t)) = (f + (g + h))(t)\]

    The zero vector is the function \(g(t) = 0\); then \((f + g)(t) = f(t) + g(t) = f(t) + 0 = f(t)\).
    The uniqueness follows from the uniqueness of the additive identity in \(\mathbb{C}\).
    \(g\) is also in \(V\) as \(g(-t) = 0 = \overline{0} = \overline{g(t)}\).

    For each vector \(f(t)\), there exists a unique vector \(-f(t)\) such that \((f + (-f))(t) = 0\):
    \[(f + (-f))(t) = f(t) + -f(t) = f(t) - f(t) = 0\]
    It is in \(V\), as \(-f(-t) = -\overline{f(t)} = \overline{-f(t)}\).

    Scalar multiplication is distributive over vector addition. Suppose \(c\) is a real number and \(f, g \in V\). Then,
    \begin{gather*}
        c(f + g)(t) \\
        = c(f(t) + g(t)) \\
        = cf(t) + cg(t) \\
        = (cf + cg)(t) \\
        = (cf)(t) + (cg)(t)
    \end{gather*}
    as multiplication is distributive over addition in \(\mathbb{C}\).

    Finally, suppose \(c_1\) and \(c_2\) are real numbers and \(f \in V\). Then
    \begin{gather*}
        ((c_1 + c_2)f)(t) \\
        = (c_1 + c_2)f(t) \\
        = c_1f(t) + c_2f(t) \\
        = (c_1f + c_2f)(t)
    \end{gather*}
\end{enumerate}

\subsection{Vector Spaces}
\begin{enumerate}[listparindent=\parindent]
\item[1.] Which of the following sets of vectors \(\alpha = (\alpha_1, \dots, \alpha_n)\)
    in \(\mathbb{R}^n\) are subspaces for \(\mathbb{R}^n (n \geq 3)\)?

    For each set below, denote the given set \(S\) and suppose \(\alpha, \beta \in S\) and \(c \in \mathbb{R}\).
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] all \(\alpha\) such that \(\alpha_1 \geq 0\);

            \(S\) is not a subspace, as \(-\alpha \notin S\) for all \(\alpha \neq 0\).

        \item[(b)] all \(\alpha\) such that \(\alpha_1 + 2\alpha_2 = \alpha_3\);
            Suppose \(\gamma = c\alpha + \beta = (\gamma_1, \gamma_2, \gamma_3, \dots)\);
            \begin{gather*}
                \gamma_1 + 2\gamma_2 \\
                = c(\alpha_1 + \beta_1) + 2c(\alpha_2 + \beta_2) \\
                = c[(\alpha_1 + 2\alpha_2) + (\beta_1 + 2\beta_2)] \\
                = c(\alpha_3 + \beta_3) \\
                = \gamma_3
            \end{gather*}
            Therefore \(\gamma \in S\) and \(S\) is a subspace by Theorem 1.

        \item[(c)] all \(\alpha\) such that \(\alpha_2 = \alpha_1^2\);

            Suppose \(\gamma = \alpha + \beta\);
            then \(\gamma_2 = \alpha_2 + \beta_2 = \alpha_1^2 + \beta_1^2 \neq (\alpha_1 + \beta_1)^2 = \gamma_1^2\).
            \(\gamma \notin S\), therefore \(S\) is not a subspace.

        \item[(d)] all \(\alpha\) such that \(\alpha_1\alpha_2 = 0\).

            Suppose \(\gamma = c\alpha + \beta = (\gamma_1, \gamma_2, \dots)\);
            \begin{gather*}
                \gamma_1\gamma_2 = c^2(\alpha_1 + \beta_1)(\alpha_2 + \beta_2) \\
                = c^2(\alpha_1\alpha_2 + \alpha_2\beta_1 + \alpha_1\beta_2 + \beta_1\beta_2) \\
                = c^2(\alpha_2\beta_1 + \alpha_1\beta_2)
            \end{gather*}
            At least one of \(\alpha_1\) and \(\alpha_2\) must be 0, and at least one of \(\beta_1\) and \(\beta_2\) must be 0.
            However, this does not guarantee that \(\alpha_2\beta_1 + \alpha_1\beta_2 = 0\), so \(\gamma \notin S\) for some \(\alpha\) and \(\beta\),
            Therefore \(S\) is not a subspace.

        \item[(e)] all \(\alpha\) such that \(\alpha_2\) is rational.

            Suppose \(\gamma = \sqrt{2}\alpha\). Then \(\gamma_2 = \sqrt{2}\alpha_2\) is not rational
            and \(\gamma \notin S\), so \(S\) is not a subspace.

    \end{enumerate}
\item[2.] Let \(V\) be the (real) vector space of all functions \(f\) from \(R\) to \(R\).
    Which of the following are subspaces of \(V\)?

    For each set below, denote the given set \(W\) and suppose \(f, g \in W\) and \(c \in \mathbb{R}\).
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] all \(f\) such that \(f(x^2) = f(x)^2\);

            Suppose \(f(x) = x\) and \(g(x) = x^2\). \(f, g \in W\) because
            \(f(x^2) = x^2 = f(x)^2\) and \(g(x^2) = x^4 = (x^2)^2 = g(x)^2\).

            Then \(h(x) = (f + g)(x) = x + x^2\), but \(h(x^2) = x^2 + x^4\) and
            \(h(x)^2 = (x + x^2)^2 = x^2 + 2x^3 + x^4\). Therefore \(h(x) \notin S\)
            and this set is not a subspace.

        \item[(b)] all \(f\) such that \(f(0) = f(1)\).

            \(h(0) = cf(0) + g(0)\) and \(h(1) = cf(1) + g(1)\).
            \(f(0) = f(1)\) and \(g(0) + g(1)\), so it must be the case that \(h(0) = h(1)\),
            and \(h \in W\). By Theorem 1, \(W\) is a subspace of \(V\).

        \item[(c)] all \(f\) such that \(f(3) = 1 + f(-5)\).

            Suppose \(h = f + g\).
            It can be shown that \(h(3) \neq 1 + h(-5)\);

            \begin{gather*}
                h(3) = f(3) + g(3) \\
                = 1 + f(-5) + 1 + g(-5) \\
                = 2 + f(-5) + g(-5) \\
                = 2 + h(-5) \\
                \neq 1 + h(-5)
            \end{gather*}

            As \(f + g \notin W\), \(W\) is not a subspace of \(V\).

        \item[(d)] all \(f\) such that \(f(-1) = 0\);

            Suppose \(h = cf + g\). Then \(h \in W\);
            \begin{gather*}
                h(-1) = cf(-1) + g(-1) \\
                = c(0) + 0 \\
                = 0
            \end{gather*}
            By Theorem 1, \(W\) is a subspace of \(V\).

        \item[(e)] all \(f\) which are continuous.

            Suppose \(h = cf + g\). It must be that \(h \in W\)
            since the sum of two continuous functions is continous,
            and if \(f\) is a continuous function then \(cf\) is a continuous function for all \(c \in \mathbb{R}\).
            By Theorem 1, \(W\) is a subspace of \(V\).
    \end{enumerate}

\item[3.] Is the vector (3, -1, 0, -1) in the subspace of \(\mathbb{R}^4\) spanned by the vectors
    (2, -1, 3, 2), (-1, 1, 1, -3), and (1, 1, 9, -5)?

    The subspace spanned by a set of vectors is the set of all linear combinations of the vectors.
    To check if the given vector is a linear combination of the three other vectors, we solve for \(X\) in
    \[
        \begin{bmatrix}
            2 & -1 & 1 \\
            -1 & 1 & 1 \\
            3 & 1 & 9 \\
            2 & -3 & -5
        \end{bmatrix}
        X
        =
        \begin{bmatrix}
            3 \\ -1 \\ 0 \\ -1
        \end{bmatrix}
    \]
    and see that there is no solution for the equation.
    As \((3, -1, 0, -1)\) is cannot be written as a linear combination of the other vectors,
    it is not in the subspace spanned by the three vectors.

\item[4.] Let \(W\) be the set of all \((x_1, x_2, x_3, x_4, x_5)\) in \(\mathbb{R}^5\) which satisfy
    \[
        \begin{system}{5}
            2x_1 &-& x_2 &+& \frac{4}{3}x_3 &-& x_4 && &=& 0 \\
            x_1 && &+& \frac{2}{3}x_3 && &-& x_5 &=& 0 \\
            9x_1 &-& 3x_2 &+& 6x_3 &-& 3x_4 &-& 3x_5 &=& 0 \\
        \end{system}
    \]
    Find a finite set of vectors which spans \(W\).

\[
    \begin{bmatrix}
        2 & -1 & \frac{4}{3} & -1 & 0 \\
        1 & 0 & \frac{2}{3} & 0 & -1 \\
        9 & -3 & 6 & -3 & -3
    \end{bmatrix}
\]

The reduced row-echelon matrix is
\[
    \begin{bmatrix}
        1 & 0 & \frac{2}{3} & 0 & -1 \\
        0 & 1 & 0 & 1 & -2 \\
        0 & 0 & 0 & 0 & 0
    \end{bmatrix}
\]

Therefore, all vectors in \(W\) are in the form
\[ (-\frac{2}{3}x_3 + x_5, -x_4 + 2x_5, x_3, x_4, x_5). \]

\{(1, 2, 0, 0, 1), (0, -1, 0, 1, 0), (-2, 0, 3, 0, 0)\}
is an example of set of vectors that span \(W\).

\item[5.] Let \(F\) be a field and let \(n\) be a positive integer \((n \geq 2)\).
    Let \(V\) be the vector space of all \(n \times n\) matrices over \(F\).
    Which of the following sets of matrices \(A\) in \(V\) are subspaces of \(V\)?
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] all invertible \(A\);

            The set of all invertible matrices is not a subspace of \(V\)
            because the zero matrix, which is the zero vector in \(V\), is not invertible.

        \item[(b)] all non-invertible \(A\);

            The set of all non-invertible matrices is not a subspace of \(V\)
            since two non-invertible matrices can sum to an invertible matrix;
            \[
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 0 \\
                \end{bmatrix}
                +
                \begin{bmatrix}
                    0 & 0 \\
                    0 & 1 \\
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1 \\
                \end{bmatrix}
            \]

        \item[(c)] all \(A\) such that \(AB = BA\), where \(B\) is some fixed matrix in \(V\);

            Let the set of all matrices \(A\) such that \(AB = BA\) be \(W\).
            Suppose \(A_1\) and \(A_2\) are matrices in \(W\). Then any matrix in the form
            \(cA_1 + A_2\) where \(c \in F\) is in \(W\);
            \begin{gather*}
                (cA_1 + A_2)B \\
                = cA_1B + A_2B \\
                = cBA_1 + BA_2 \\
                = B(cA_1) + BA_2 \\
                = B(cA_1 + A_2) \\
            \end{gather*}

            By Theorem 1, the set of all \(A\) such that \(AB = BA\) is a subspace of \(V\).

        \item[(d)] all \(A\) such that \(A^2 = A\).

            Suppose \(I\) is the \(2 \times 2\) identity matrix;
            \[
                I + I
                =
                \begin{bmatrix}
                    2 & 0 \\
                    0 & 2 \\
                \end{bmatrix}
            \]

            Clearly, \(I^2 = I\). However
            \[
                \begin{bmatrix}
                    2 & 0 \\
                    0 & 2 \\
                \end{bmatrix}^2
                =
                \begin{bmatrix}
                    4 & 0 \\
                    0 & 4 \\
                \end{bmatrix}
            \]

            As it is not closed under addition, the set is not a subspace of \(V\).
    \end{enumerate}

\item[6.]
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Prove that the only subspaces of \(\mathbb{R}^1\) are \(\mathbb{R}^1\) and the zero subspace.

            First, any space trivially has the zero subspace as its subspace.
            Suppose \(S\) be a subspace of \(\mathbb{R}^1\) that is not the zero subspace.
            \(S\) is the set of all linear combinations \(cv\) for some non-zero vector \(v = (v_1)\) in \(S\) and some scalar \(c\).
            Then vector \(u = (u_1)\) in \(\mathbb{R}^1\) must be in \(S\), since a scalar \(c = \frac{u_1}{v_1}\) exists
            such that \(u_1 = cv_1\) for all \(v_1 \neq 0\). Therefore, \(S\) = \(\mathbb{R}^1\).

        \item[(b)] Prove that a subspaces of \(\mathbb{R}^2\) is \(\mathbb{R}^2\), or the zero subspace,
            or consists of all scalar multiples of some fixed vector in \(\mathbb{R}^2\).
            (The last type of subspace is, intuitively, a straight line through the origin).

            Suppose \(v, u \in \mathbb{R}^2\) and \(v \neq 0\); if \(v = 0\) it can only span the trivial zero subspace.

            If \(v\) and \(u\) are scalar multiples of each other, they must span the subspace of
            set of all scalar multiples of some fixed vector in \(\mathbb{R}^2\).
            The space spanned by \(\{v\}\) is \(\{cv \mid c \in \mathbb{R}\}\).
            If \(u\) is scalar multiple of \(v\), then all scalar multiples of \(u\) are scalar multiples of \(v\) as well
            and therefore both \(\{u, v\}\) and \(\{v\}\) span the set of all scalar multiples of \(v\).

            Otherwise, if \(v\) and \(u\) are not scalar multiples of each other, they must span \(\mathbb{R}^2\).
            Let \(w\) be some linear combination of \(v = (v_1, v_2)\) and \(u = (u_1, u_2)\);
            \begin{gather*}
                \begin{bmatrix}
                    v_1 & u_1 \\
                    v_2 & u_2
                \end{bmatrix}
                X
                =
                w
            \end{gather*}

            Since \(v \neq 0\), suppose \(v_1 \neq 0\). Then
            \begin{gather*}
                \begin{bmatrix}
                    v_1 & u_1 \\
                    v_2 & u_2
                \end{bmatrix}
                \rightarrow
                \begin{bmatrix}
                    v_1 & u_1 \\
                    0 & \frac{v_1u_2 - u_1v_2}{v_1}
                \end{bmatrix}
            \end{gather*}

            If \(u\) is not a constant multiple of \(v\),
            then \((u_1, u_2) = (av_1, bv_2), a \neq b\) and \(v_1u_2 - u_1v_2 = au_1u_2 - bu_1u_2 \neq 0\).
            Then it's clear that the matrix is invertible and there must be a solution for all \(w\).
            If \(v_1 = 0\), then \(v_2 \neq 0\), and similar logic holds after swapping the two rows.
            This means vectors in \(\mathbb{R}^2\) can be written as a linear combination of \(v\) and \(u\),
            hence \(\{v, u\}\) must span \(\mathbb{R}^2\).

            (TODO: There's probably a more elegant way of doing this)

        \item[(c)] Can you describe the subspaces of \(\mathbb{R}^3\)?

            The only subspaces of \(\mathbb{R}^3\) are the zero subspace, \(\{cv \mid c \in \mathbb{R}\}\) for some \(v \in \mathbb{R}^3\),
            \(\{c_1v_1 + c_2v_2 \mid c_1, c_2 \in \mathbb{R}\}\) where \(v_1, v_2 \in \mathbb{R}^3\), and \(\mathbb{R}^3\) itself.

            (TODO)
    \end{enumerate}

\item[7.] Let \(W_1\) and \(W_2\) be subspaces of a vector space \(V\) such that the set-theoretic union of \(W_1\) and \(W_2\) is also a subspace.
    Prove that one of the spaces \(W_i\) is contained in the other.

    Suppose \(W_1 \cup W_2\) is a subspace, but \(W_2 \not\subset W_1\) and \(W_1 \not\subset W_2\).
    There is some vector \(v \in W_1\) such that \(v \not\in W_2\) and
    some vector \(u \in W_2\) such that \(u \not\in W_1\).
    Since vector spaces are closed under addition, \(v + u \in W_1 \cup W_2\), and either \(v + u \in W_1\) or \(v + u \in W_2\).
    This implies \(v + u + (-v) = u \in W_1\) or \(v + u + (-u) = v \in W_2\), which is a contradiction.
    Therefore, either \(W_1 \subset W_2\) or \(W_2 \subset W_1\).

\item[8.] Let \(V\) be the vector space of all functions from \(R\) into \(R\);
    let \(V_e\) be the subset of even functions, \(f(-x) = f(x)\);
    let \(V_o\) be the subset of odd functions, \(f(-x) = -f(x)\).

    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Prove that \(V_e\) and \(V_o\) are subspaces of \(V\).

            Suppose \(f, g \in V_e\), \(c \in \mathbb{R}\). Then
            \begin{gather*}
                (cf + g)(-x) \\
                = cf(-x) + g(-x) \\
                = cf(x) + g(x) \\
                = (cf(x) + g(x) \\
                = (cf + g)(x)
            \end{gather*}
            Since \((cf + g)(-x) = (cf + g)(x)\), \(V_e\) is closed under addition and multiplication
            and therefore is a subspace of \(V\).

            Likewise, suppose \(f, g \in V_o\), \(c \in \mathbb{R}\). Then
            \begin{gather*}
                (cf + g)(-x) \\
                = cf(-x) + g(-x) \\
                = -cf(x) + (-g(x)) \\
                = -(cf(x) + g(x)) \\
                = -(cf + g)(x)
            \end{gather*}
            Since \((cf + g)(-x) = -(cf + g)(x)\), \(V_o\) is closed under addition and multiplication
            and therefore is a subspace of \(V\).

        \item[(b)] Prove that \(V_e + V_o = V\).

            Any function \(f \in V\) can be written as a sum of functions \(g \in V_e\) and \(h \in V_o\);
            \begin{gather*}
                f(x) = g(x) + h(x) \\
                f(-x) = g(x) - h(x)
            \end{gather*}
            Add and subtract the two equations to get
            \begin{gather*}
                \frac{f(x) + f(-x)}{2} = g(x) \\
                \frac{f(x) - f(-x)}{2} = h(x)
            \end{gather*}
            which is defined for all \(f\).
            Therefore, any function in \(V\) is a sum of functions in \(V_e\) and \(V_o\); \(V = V_e + V_o\).

        \item[(c)] Prove that \(V_e \cap V_o = \{0\}\).

            If \(f \in V_e \cap V_o\), then \(f(-x) = f(x)\) and \(f(-x) = -f(x)\). Then
            \begin{gather*}
                f(x) = -f(x) \\
                2f(x) = 0 \\
                f(x) = 0
            \end{gather*}
    \end{enumerate}

\item[9.] Let \(W_1\) and \(W_2\) be subspaces of a vector space \(V\) such that \(W_1 + W_2 = V\)
    and \(W_1 \cup W_2 = \{0\}\). Prove that for each vector \(\alpha\) in \(V\) there are
    \textit{unique} vectors \(\alpha_1\) in \(W_1\) and \(\alpha_w\) in \(W_2\) such that \(\alpha = \alpha_1 + \alpha_2\).

    Suppose \(\alpha = \alpha_1 + \alpha_2\ = \beta_1 + \beta_2\) where \(\alpha_1, \beta_1 \in W_1\) and \(\alpha_2, \beta_2 \in W_2\).
    \begin{gather*}
        \alpha_1 - \beta_1 + \alpha_2 - \beta_2 = 0 \\
        \alpha_1 - \beta_1 = -(\alpha_2 - \beta_2) \\
        \alpha_1 - \beta_1 \in W_1 \\
        \beta_2 - \alpha_2 \in W_2 \\
        \implies \alpha_1 - \beta_1, \beta_2 - \alpha_2 \in W_1 \cap W_2
    \end{gather*}
    \(W_1 \cap W_2 = \{0\}\), so \(\alpha_1 - \beta_1 = \beta_2 - \alpha_2 = 0\)
    which must mean \(\alpha_1 = \beta_1\) and \(\alpha_2 = \beta_2\).
    Therefore, \(\alpha_1\) and \(\alpha_2\) are unique.
\end{enumerate}

\subsection{Bases and Dimension}
\begin{enumerate}[listparindent=\parindent]
\item[1.] Prove that if two vectors are linearly dependent, one of them is a scalar multiple of the other.

    Suppose \(\alpha\) and \(\beta\) are linearly dependent. Then there exist scalars \(a\) and \(b\), where both are not 0,
    such that \(a\alpha + b\beta = 0\). Then \(a\alpha = -b\beta\) and \(\alpha = \frac{-b}{a}\beta\) or \(\frac{-a}{b}\alpha = \beta\).

\item[2.] Are the vectors
    \begin{gather*}
        \alpha_1 = (1, 1, 2, 4),\quad \alpha_2 = (2, -1, -5, 2) \\
        \alpha_3 = (1, -1, -4, 0),\quad \alpha_4 = (2, 1, 1, 6)
    \end{gather*}
    linearly independent in \(\mathbb{R}^4\)?

The matrix where the row vectors are the four given vectors,
\[
    \begin{bmatrix}
        1 & 1 & 2 & 4 \\
        2 & -1 & -5 & 2 \\
        1 & -1 & -4 & 0 \\
        2 & 1 & 1 & 6
    \end{bmatrix},
\]
is invertible, therefore the given vectors form a linearly independent set by the converse of Corollary 3 to Theroem 5.

The text never proves the converse, so here is a proof;
If an \(n \times n\) matrix \(A\) with the row vectors \(\alpha_1, \alpha_2, \dots, \alpha_n\) is invertible,
\(AX = 0\) only has the trivial solution \(X = 0\).
So a linear combination of the row vectors of \(A\) is 0 only if every scalar is 0, meaning they must be linearly independent.

\item[3.] Find a basis for the subspace of \(\mathbb{R}^4\) spanned by the vectors of Exercise 2.

The matrix given by the four vectors,
\[
    \begin{bmatrix}
        1 & 1 & 2 & 4 \\
        2 & -1 & -5 & 2 \\
        1 & -1 & -4 & 0 \\
        2 & 1 & 1 & 6
    \end{bmatrix},
\]
is row-equivalent to
\[
    \begin{bmatrix}
        1 & 1 & 2 & 4 \\
        0 & -3 & -9 & 6 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0
    \end{bmatrix}.
\]

The row vectors are linear combinations of the row vectors of the original matrix,
and therefore the two sets both span the same subspace.
Ignoring the zero vectors, the two vectors are not linearly dependent as proven by Exercise 1
and \(\{(1, 1, 2, 4), (0, -3, -9, 6)\}\) span the given subspace.

\item[4.] Show that the vectors
    \[
        \alpha_1 = (1, 0, -1),\quad \alpha_2 = (1, 2, 1),\quad \alpha_3 = (0, -3, 2)
    \] form a basis for \(\mathbb{R}^3\). Express each of the standard basis vectors as linear combinations of \(\alpha_1, \alpha_2, \alpha_3\).

    Let
\[
    AX =
    \begin{bmatrix}
        1 & 1 & 0 \\
        0 & 2 & -3 \\
        -1 & 1 & 2
    \end{bmatrix}
    X = Y,
\] where \(X, Y \in \mathbb{R}^3\).
Since \(A\) is invertible, there is a solution \(X\) for any \(Y\);
which implies all vectors in \(\mathbb{R}^3\) are a linear combination of the column vectors of \(A\).
Therefore \(\{\alpha_1, \alpha_2, \alpha_3\}\) is a basis of \(\mathbb{R}^3\).

Omitting the steps of finding \(A^{-1}\) because no one wants to look at a page of Gaussian elimination,
\[
    X = A^{-1}Y =
    \begin{bmatrix}
        \frac{7}{10} & \frac{-1}{5} & \frac{-3}{10} \\
        \frac{3}{10} & \frac{1}{5} & \frac{3}{10} \\
        \frac{1}{5} & \frac{-1}{5} & \frac{1}{5}
    \end{bmatrix}Y
\]

\begin{align*}
    (1, 0, 0) &= \frac{7}{10}\alpha_1 - \frac{3}{10}\alpha_2 + \frac{1}{5}\alpha_3 \\
    (0, 1, 0) &= \frac{-1}{5}\alpha_1 - \frac{1}{5}\alpha_2 + \frac{-1}{5}\alpha_3 \\
    (0, 0, 1) &= \frac{-3}{10}\alpha_1 - \frac{3}{10}\alpha_2 + \frac{1}{5}\alpha_3
\end{align*}

\item[5.] Find three vectors in \(\mathbb{R}^3\) which are linearly dependent,
    and are such that any two of them are linearly independent.

Take two linearly independent vectors and form a linear combination of them;
\(\{(1, 0, 0), (0, 1, 0), (1, 1, 0)\}\) one example.
No two vectors are scalar multiples of each other and therefore linearly independent, but
\((1, 1, 0) - (1, 0, 0) - (1, 1, 0) = (0, 0, 0)\),

\item[6.] Let \(V\) be the vector space of all \(2 \times 2\) matrices over the field \(F\).
    Prove that \(\dim V = 4\) by exhibiting a basis for \(V\) which as four elements.

\[
    \{
        \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix},
        \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix},
        \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix},
        \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
    \}
\]
spans \(V\);
\[
    a \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} +
    b \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} +
    c \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} +
    d \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
    =
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix},
\]
which descrbies all matrices in \(V\), and their linear combinations is the zero matrix only if all scalar are zero.

\item[7.] Let \(V\) be the vector space of Exercise 6. Let \(W_1\) be the set of matrices of the form
    \[
        \begin{bmatrix}
            x & -x \\
            y & z
        \end{bmatrix}
    \]
    and let \(W_2\) be the set of matrices of the form
    \[
        \begin{bmatrix}
            a & b \\
            -a & c
        \end{bmatrix}
    \]

\begin{enumerate}[listparindent=\parindent]
    \item[(a)] Prove that \(W_1\) and \(W_2\) are subspaces of \(V\).

    \(W_1\) is closed under scalar multiplication and vector addition;
    \[
        \alpha
        \begin{bmatrix}
            x_1 & -x_1 \\
            y_1 & z_1
        \end{bmatrix}
        +
        \begin{bmatrix}
            x_2 & -x_2 \\
            y_2 & z_2
        \end{bmatrix}
        =
        \begin{bmatrix}
            \alpha x_1 + x_2 & -(\alpha x_1 + x_2) \\
            \alpha y_1 + y_2 & \alpha z_1 + z_2
        \end{bmatrix}
    \]

    So is \(W_2\);
    \[
        \alpha
        \begin{bmatrix}
            a_1 & b_1 \\
            -a_1 & c_1
        \end{bmatrix}
        +
        \begin{bmatrix}
            a_2 & b_2 \\
            -a_2 & c_2
        \end{bmatrix}
        =
        \begin{bmatrix}
            \alpha a_1 + a_2 & \alpha b_1 + b_2 \\
            -(\alpha a_1 + a_2) & \alpha c_1 + c_2
        \end{bmatrix}
    \]

    \item[(b)] Find the dimensions of \(W_1, W_2, W_1 + W_2\) and \(W_1 \cap W_2\).

    \(\dim W_1 = 3\) because
    \[
        \{
            \begin{bmatrix} 1 & -1 \\ 0 & 0 \end{bmatrix},
            \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix},
            \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
        \}
    \]
    spans \(W_1\);
    \[
        x \begin{bmatrix} 1 & -1 \\ 0 & 0 \end{bmatrix} +
        y \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} +
        z \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
        =
        \begin{bmatrix}
            x & -x \\
            y & z
        \end{bmatrix}
        \in W_1
    \]
    which is equal to zero matrix only if all scalars are 0.

    Similarly, \(\dim W_2 = 3\);
    \[
        a \begin{bmatrix} 1 & 0 \\ -1 & 0 \end{bmatrix} +
        b \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} +
        c \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
        =
        \begin{bmatrix}
            a & b \\
            -a & c
        \end{bmatrix}
        \in W_2
    \]
    which is equal to zero matrix only if all scalars are 0.

    \(W_1 \cap W_2\) is the set of all matrices in the form
    \[
        \begin{bmatrix}
            x & -x \\
            -x & y
        \end{bmatrix}
    \] and its dimension is 2;

    \[
        x \begin{bmatrix} 1 & -1 \\ -1 & 0 \end{bmatrix} +
        y \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
        =
        \begin{bmatrix}
            x & -x \\
            -x & y
        \end{bmatrix}
        \in W_1 \cap W_2
    \]
    which is equal to zero matrix only if all scalars are 0.

    Then \(\dim W_1 + W_2 = \dim W_1 + \dim W_2 - \dim W_1 \cap W_2 = 4\).
\end{enumerate}

\item[8.] Again let \(V\) be the space of \(2 \times 2\) matrices over \(F\).
    Find a basis \(\{A_1, A_2,\allowbreak A_3, A_4\}\) for \(V\) such that \(A_j^2 = A_j\) for each \(j\).

Let \(A_1 = \lbrack\begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix}\rbrack\)
and \(A_2 = \lbrack\begin{smallmatrix} 0 & 0 \\ 0 & 1 \end{smallmatrix}\rbrack\).
\(A_1^2 = A_1, A_2^2 = A_2\), and they are linearly independent so \(\{A_1, A_2\}\) is a subset of some basis of \(V\).

Then let \(A_3 = \lbrack\begin{smallmatrix} 1 & 1 \\ 0 & 0 \end{smallmatrix}\rbrack\).
\(A_3^2 = A_3\) and \({A_3} \cup \{A_1, A_2\}\) is linearly independent because
\(A_3\) is not in the span of \(\{A_1, A_2\}\); \((aA_1 + bA_2)_{1,2} = 0\) for all \(a, b \in F\) but \((A_3)_{1,2} = 1\).

Similarly, let \(A_4 = \lbrack\begin{smallmatrix} 0 & 0 \\ 1 & 1 \end{smallmatrix}\rbrack\).
\(A_4^2 = A_4\) and \({A_4} \cup \{A_1, A_2, A_3\}\) is linearly independent because
\((aA_1 + bA_2 + cA_3)_{2,1} = 0\) for all \(a, b, c \in F\) but \((A_4)_{2,1} = 1\).

Since it is a linearly independent set, \(\{A_1, A_2, A_3, A_4\}\) is a basis of \(V\).

\item[9.] Let \(V\) be a vector space over a subfield \(F\) of the complex numbers.
    Suppose \(\alpha\), \(\beta\), and \(\gamma\) are linearly independent vectors in \(V\).
    Prove that \(\alpha + \beta\), \(\beta + \gamma\), and \(\gamma + \alpha\) are linearly independent.

    \begin{gather*}
    a(\alpha + \beta) + b(\beta + \gamma) + c(\gamma + \alpha) = 0\\
    (a + c)\alpha + (a + b)\beta + (b + c)\gamma = 0
    \end{gather*}

    This implies \(a + c = a + b = b + c = 0\) because \(\alpha\), \(\beta\), and \(\gamma\) are linearly independent.
    So
    \[
        \begin{bmatrix}
            1 & 0 & 1 \\
            1 & 1 & 0 \\
            0 & 1 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
            a \\ b \\ c
        \end{bmatrix}
        = 0
    \]

    The coefficient matrix is invertible;
    \[
        \begin{bmatrix}
            1 & 0 & 1 \\
            1 & 1 & 0 \\
            0 & 1 & 1 \\
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 1 \\
            0 & 1 & -1 \\
            0 & 1 & 1 \\
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 1 \\
            0 & 1 & -1 \\
            0 & 0 & 2 \\
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1 \\
        \end{bmatrix}
    \]

    So only solution is \(a = b = c = 0\) and \(\alpha + \beta\), \(\beta + \gamma\), and \(\gamma + \alpha\) must be linearly independent.

\item[10.] Let \(V\) be a vector space over the field \(F\). Suppose there are a finite number of vectors \(\alpha_1, \dots, \alpha_r\) in \(V\) which span \(V\).
    Prove that \(V\) is finite-dimensional.

    By Theorem 4, since a finite set spans \(V\), any independent set of vectors in \(V\) is finite.
    Bases are independent by definition, so any basis of \(V\) is a finite set and \(V\) must be finite-dimensional.

\item[13.] Discuss Exercise 9, when \(V\) is a vector space over the field \(F\) with two elements described in Exercise 5, Section 1.1.

    \(F\) has two elements, denoted 0 and 1, where \(a + b = 1\) if and only if exactly one of \(a\) and \(b\) is 1, and \(a \cdot b = 1\) if and only if both \(a\) and \(b\) are 1.
    \(0 + 1 = 1\) and \(0 + 0 = 0\), so the zero scalar in \(F\) is 0.

    Suppose \(\alpha\), \(\beta\), \(\gamma\) are linearly independent vectors in \(V\).
    Then \(\alpha + \beta\), \(\beta + \gamma\), and \(\gamma + \alpha\) are linearly dependent;
    \begin{gather*}
    1(\alpha + \beta) + 1(\beta + \gamma) + 1(\gamma + \alpha) \\
    = (1 + 1)\alpha + (1 + 1)\beta + (1 + 1)\gamma \\
    = 0\alpha + 0\beta + 0\gamma
    = 0
    \end{gather*}

\end{enumerate}

\subsection{Coordinates}
\begin{enumerate}[listparindent=\parindent]

\item[1.] Show that the vectors
    \begin{gather*}
        \alpha_1 = (1, 1, 0, 0),\quad \alpha_2 = (0, 0, 1, 1) \\
        \alpha_3 = (1, 0, 0, 4),\quad \alpha_4 = (0, 0, 0, 2)
    \end{gather*}
    form a basis for \(\mathbb{R}^4\). Find the coordinates of each of the standard basis vectors in the ordered basis \(\{\alpha_1, \alpha_2, \alpha_3, \alpha_4\}\).

    Let \[P = \begin{bmatrix}
        1 & 0 & 1 & 0 \\
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 4 & 2
    \end{bmatrix}\]

    \(P\) is invertible and its inverse is
    \[\begin{bmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        1 & -1 & 0 & 0 \\
        -2 & 2 & \frac{-1}{2} & \frac{1}{2}
    \end{bmatrix}\]
    so the given vectors form a basis.

    Let \(\mathcal{B}\) be the standard basis, then \([a]_\mathcal{B'} = P^{-1}[\alpha]_\mathcal{B}\), and
    \begin{gather*}
        [(1, 0, 0, 0)]_\mathcal{B'} = \begin{bmatrix} 0 \\ 0 \\ 1 \\ -2 \end{bmatrix} \quad
        [(0, 1, 0, 0)]_\mathcal{B'} = \begin{bmatrix} 1 \\ 0 \\ -1 \\ -2 \end{bmatrix} \\
        [(0, 0, 1, 0)]_\mathcal{B'} = \begin{bmatrix} 0 \\ 1 \\ 0 \\ \frac{-1}{2} \end{bmatrix} \quad
        [(0, 0, 0, 1)]_\mathcal{B'} = \begin{bmatrix} 1 \\ 0 \\ -1 \\ -\frac{1}{2} \end{bmatrix}
    \end{gather*}

\item[2.] Find the coordinate matrix of the vector \((1, 0, 1)\) in the basis of \(\mathbb{C}^3\)
    consisting of the vectors \((2i, 1, 0)\), \((2, -1, 1)\), and \((0, 1 + i, 1 - i)\), in that order.

    If \(\mathcal{B}\) is the given basis,
    \[
        [(1, 0, 1)]_\mathcal{B} =
        P^{-1}\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} =
        \boxed {
            \begin{bmatrix} \frac{-1 - i}{2} \\ \frac{i}{2} \\ \frac{3 + i}{4} \end{bmatrix}
        }
    \]
    where
    \[
        P = \begin{bmatrix}
            2i & 2 & 0 \\
            1 & -1 & 1 + i \\
            0 & 1 & 1 - 1
        \end{bmatrix} \\
    \]

\item[3.] Let \(\mathcal{B} = \{\alpha_1, \alpha_2, \alpha_3\}\) be the ordered basis for \(\mathbb{R}^3\) consisting of
    \[
        \alpha_1 = (1, 0, -1),\quad \alpha_2 = (1, 1, 1),\quad \alpha_3 = (1, 0, 0).
    \]
    What are the coordinates of the vector \((a, b, c)\) in the ordered basis \(\mathcal{B}\)?

    \begin{gather*}
        P = \begin{bmatrix}
            1 & 1 & 1 \\
            0 & 1 & 0 \\
            -1 & 1 & 0
        \end{bmatrix}, \quad
        [a]_\mathcal{B} = P^{-1}\begin{bmatrix} a \\ b \\ c \end{bmatrix}
        = \begin{bmatrix}
            0 & 1 & -1 \\
            0 & 1 & 0 \\
            1 & -2 & 1
        \end{bmatrix}
        \begin{bmatrix} a \\ b \\ c \end{bmatrix} \\
        = \begin{bmatrix} b - c \\ b \\ a - 2b + c \end{bmatrix}
    \end{gather*}

\item[4.] Let \(W\) be the subspace of \(\mathbb{C}^3\) spaned by \(\alpha_1 = (1, 0, i)\) and \(\alpha_2 = (1 + i, 1, -1)\).

    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Show that \(\alpha_1\) and \(\alpha_2\) form a basis for \(W\).

            \(\alpha_1\) and \(\alpha_2\) are not scalar multiples of another, so they are linearly independent.
            Since it spans \(W\) and is linearly independent, \(\{\alpha_1, \alpha_2\}\) is a basis for \(W\).

        \item[(b)] Show that the vectors \(\beta_1 = (1, 1, 0)\) and \(\beta_2 = (1, i, 1 + i)\) are in \(W\) and form another basis for \(W\).

            \begin{gather*}
                \begin{bmatrix}
                    1 & 1 + i \\
                    0 & 1 \\
                    i & -1
                \end{bmatrix}
                \begin{bmatrix}
                    c_1 \\ c_2
                \end{bmatrix}
                =
                \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \\
                \begin{abmatrix}{2}{1}
                    1 & 1 + i & 1 \\
                    0 & 1 & 1 \\
                    i & -1 & 0
                \end{abmatrix}
                \rightarrow
                \begin{abmatrix}{2}{1}
                    1 & 0 & -i \\
                    0 & 1 & 1 \\
                    0 & 0 & 0
                \end{abmatrix} \\
                -i(1, 0, i) + (1 + i, 1, -1) = (1, 1, 0) \\
                -i\alpha_1 + \alpha_2 = \beta_1
            \end{gather*}

            Similarly,
            \begin{gather*}
                \begin{abmatrix}{2}{1}
                    1 & 1 + i & 1 \\
                    0 & 1 & i \\
                    i & -1 & 1 + i
                \end{abmatrix}
                \rightarrow
                \begin{abmatrix}{2}{1}
                    1 & 0 & 2 - i \\
                    0 & 1 & i \\
                    0 & 0 & 0
                \end{abmatrix} \\
                (2 - i)(1, 0, i) + i(1 + i, 1, -1) = (2 - i, i, 1 + i) \\
                (2 - i)\alpha_1 + i\alpha_2 = \beta_2
            \end{gather*}

            \(\beta_1\) and \(\beta_2\) are not scalar multiples, so they are linearly independent.
            \(\dim W = 2\) because \(\{\alpha_1, \alpha_2\}\) is a basis for \(W\), so \(\{\beta_1, \beta_2\}\) is a basis for \(W\).
    \end{enumerate}

\item[5.] Let \(\alpha = (x_1, x_2)\) and \(\beta = (y_1, y_2)\) be the vectors in \(\mathbb{R}^2\) such that
    \[
        x_1y_1 + x_2y_2 = 0, \quad x_1^2 + x_2^2 = y_1^2 + y_2^2 = 1
    \]
    Prove that \(\mathcal{B} = (\alpha, \beta)\) is a basis for \(\mathbb{R}^2\). Find the coordinates of the vector \((a, b)\)
    in the ordered basis \(\mathcal{B} = \{\alpha, \beta\}\). (The conditions on \(\alpha\) and \(\beta\) say, geometrically,
    that \(\alpha\) and \(\beta\) are perpendicular and each has length 1.)

    To show \(\mathcal{B}\) is a basis, we just need to show \(\alpha\) and \(\beta\) are linearly independent.
    Assume they are linearly independent. Then \(\beta = c\alpha\) for some scalar \(c\). Then
    \begin{gather*}
        x_1y_1 + x_2y_2 = 0 \\
        c(x_1^2 + x_2^2) = 0
    \end{gather*}
    \(x_1^2 + x_2^2 = 0\) contradicts the second condition, so \(c = 0\) and \(\beta\) must be the zero vector,
    but that is also a contradiction since \(y_1^2 + y_2^2 = 0 \neq 1\). Therefore, \(\alpha\) and \(\beta\) are linearly independent,
    and \(\mathcal{B}\) must be a basis for \(\mathbb{R}^2\).

    To find the coordinates of \((a, b)\) in \(\mathbb{R}^2\),
    \begin{gather*}
        \begin{bmatrix}
            x_1 & y_1 \\
            x_2 & y_2 \\
        \end{bmatrix}
        \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} =
        \begin{bmatrix} a \\ b \end{bmatrix} \\
        \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} =
        \begin{bmatrix}
            x_1 & y_1 \\
            x_2 & y_2 \\
        \end{bmatrix}^{-1}
        \begin{bmatrix} a \\ b \end{bmatrix}
    \end{gather*}
    \(\alpha\) and \(\beta\) cannot be zero vectors and both \(x_1\) and \(y_1\) cannot be zero otherwise \(\alpha\) and \(\beta\) are linearly dependent.
    So we can assume \(x_1 \neq 0\) and \(y_2 \neq 0\); simply flip the two rows otherwise.
    \(x_1y_2 - x_2y_1 \neq 0\), otherwise \(x_1y_2 = x_2y_1\) and \(\frac{x_1}{y_1} = \frac{x_2}{y_2}\)
    which implies the two vectors are linearly dependent.
    \begin{gather*}
        \begin{abmatrix}{2}{2}
            x_1 & y_1 & 1 & 0 \\
            x_2 & y_2 & 0 & 1 \\
        \end{abmatrix}
        \rightarrow
        \begin{abmatrix}{2}{2}
            x_1 & y_1 & 1 & 0 \\
            0 & \frac{x_1y_2 - x_2y_1}{x_1} & \frac{-x_2}{x_1} & 1 \\
        \end{abmatrix}
        \rightarrow \\
        \begin{abmatrix}{2}{2}
            x_1 & y_1 & 1 & 0 \\
            0 & 1 & \frac{-x_2}{x_1y_2 - x_2y_1} & \frac{x_1}{x_1y_2 - x_2y_1} \\
        \end{abmatrix}
        \rightarrow
        \begin{abmatrix}{2}{2}
            x_1 & 0 & \frac{x_1y_2}{x_1y_2 - x_2y_1} & \frac{-x_1y_1}{x_1y_2 - x_2y_1} \\
            0 & 1 & \frac{-x_2}{x_1y_2 - x_2y_1} & \frac{x_1}{x_1y_2 - x_2y_1} \\
        \end{abmatrix}
        \rightarrow \\
        \begin{abmatrix}{2}{2}
            1 & 0 & \frac{y_2}{x_1y_2 - x_2y_1} & \frac{-y_1}{x_1y_2 - x_2y_1} \\
            0 & 1 & \frac{-x_2}{x_1y_2 - x_2y_1} & \frac{x_1}{x_1y_2 - x_2y_1} \\
        \end{abmatrix}
    \end{gather*}

    So \[
        \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} =
        \frac{1}{x_1y_2 - x_2y_1}
        \begin{bmatrix}
            ay_2 - by_1 \\
            bx_1 - ax_2
        \end{bmatrix} \\
        = [(a, b)]_\mathcal{B}
    \]

\item[6.] Let \(V\) be the vector space over the complex numbers of all functions from \(R\) into \(C\), i.e.,
    the space of all complex-valued functions on the real line. Let \(f_1(x) = 1\), \(f_2(x) = e^{ix}\), and \(f_3(x) = e^{-ix}\).
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Prove that \(f_1\), \(f_2\), and \(f_3\) are linearly independent.
            \begin{gather*}
                af_1(x) + bf_2(x) + cf_3(x) = f_0(x) \\
                a + be^{ix} + ce^{ix} = 0 \\
                ae^{ix} + be^{2ix} + c = 0
            \end{gather*}
            This is either a quadratic function or a linear function in \(e^{ix}\), or a constant function \(f(x) = c\).
            It only equals the zero function if \(a = b = c = 0\). Therefore \(f_1\), \(f_2\), and \(f_3\) are linearly independent.

        \item[(b)] Let \(g_1(x) = 1, g_2(x) = \cos x, g_3(x) = \sin x\). Find an invertible \(3 \times 3\) matrix \(P\) such that
            \[g_j = \sum_{i = 1}^3 P_{ij}f_i\]

        \begin{gather*}
            e^{ix} = \cos x + i\sin x \\
            e^{-ix} = \cos x - i\sin x \\
            \\
            1 = 1(1) + 0(e^{ix}) + e^{-ix} \\
            \cos x = 0(1) + \frac{1}{2}e^{ix} + \frac{1}{2}e^{-ix} \\
            \sin x = 0(1) - \frac{i}{2}e^{ix} + \frac{i}{2}e^{-ix} \\
            P = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & \frac{1}{2} & \frac{-i}{2} \\
                0 & \frac{1}{2} & \frac{i}{2} \\
            \end{bmatrix}
        \end{gather*}
        P is invertible;
        \begin{gather*}
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & \frac{1}{2} & \frac{-i}{2} \\
                0 & \frac{1}{2} & \frac{i}{2} \\
            \end{bmatrix}
            \rightarrow
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & -i \\
                0 & 1 & i \\
            \end{bmatrix}
            \rightarrow
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & -i \\
                0 & 0 & 2i \\
            \end{bmatrix}
            \rightarrow \\
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & -i \\
                0 & 0 & 1 \\
            \end{bmatrix}
            \rightarrow
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 1 \\
            \end{bmatrix}
        \end{gather*}

    \end{enumerate}

\item[7.] Let \(V\) be the (real) vector space of all polynomial functions from \(\mathbb{R}\) into \(\mathbb{R}\)
    of degree 2 or less, i.e., the space of all functions \(f\) of the form
    \[ f(x) = c_0 + c_1x + c_2x^2. \]
    Let \(t\) be a fixed real number and define
    \[g_1(x) = 1, \quad g_2(x) = x + t, \quad g_3(x) = (x + t)^2.\]
    Prove that \(\mathcal{B} = \{g_1, g_2, g_3\}\) is a basis for \(V\). If
    \[ f(x) = c_0 + c_1x + c_2x^2 \] what are the coordinates of \(f\) in this ordered basis \(\mathcal{B}\)?

    \(\dim V = 3\) since the set \(\{h_1, h_2, h_3\}\) where \(h_1(x) = 1\), \(h_2(x) = x\), \(h_3(x) = x^2\) spans \(V\),
    and is linearly independent; \(c_0 + c_1x + c_2x^2 = 0 \) for all \(x\) if and only if \(c_0 = c_1 = c_2 = 0\).
    Since it has 3 elements, if \(\mathcal{B}\) is linearly independent, then it is a basis of \(V\).
    \begin{gather*}
        d_0 + d_1(x + t) + d_2(x + t)^2 = 0 \\
        (d_0 + d_1t + d_2t^2) + (d_1 + 2d_2t)x + d_2x^2 = 0
    \end{gather*}
    Again, this is only true for all \(x\) if and only if \(d_0 = d_1 = d_2 = 0\). Therefore \(\mathcal{B}\) is a basis of \(V\).
    \begin{gather*}
        d_0 + d_1(x + t) + d_2(x + t)^2 = c_0 + c_1x + c_2x^2 \\
        (d_0 + d_1t + d_2t^2) + (d_1 + 2d_2t)x + d_2x^2 = c_0 + c_1x + c_2x^2 \\
        \\
        c_2 = d_2,\quad c_1 = d_1 + 2d_2t,\quad c_0 = d_0 + d1_t + d_2t^2 \\
        d_1 = c_1 - 2d_2t = c_1 - 2c_2t \\
        d_0 = c_0 - d_1t - d_2t^2 = c_0 - (c_1 - 2c_2t)t - c_2t^2 = c_0 - c_1 + c_2t^2
    \end{gather*}
    The coordinates of \(f\) in \(\mathcal{B}\) are \((c_0 - c_1t + c_2t^2, c_1 - 2c_2t, c_2)\).

\end{enumerate}

\setcounter{subsection}{6}
\subsection{Computations Concerning Subspaces}

\begin{enumerate}[listparindent=\parindent]
\item[1.] Let \(s < n\) and \(A\) an \(s \times n\) matrix with entries in the field \(F\). Use Theorem 4 (not its proof)
    to show that there is a non-zero \(X\) in \(F^{n \times 1}\) such that \(AX = 0\).

    There are \(n\) column vectors of \(A\), \(\alpha_1, \dots, \alpha_n\),
    which span a subspace of \(F^s\) and therefore must have a dimension of at most \(s\).
    By Theorem 4, the set of column vectors must be linearly dependent since it contains more than \(s\) elements.
    Then there exists nonzero scalars \(x_1, \dots, x_n\) such that \(x_1\alpha_1 + \dots + x_n\alpha_n = 0\).
    If
    \[X = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \]
    then \(AX = x_1\alpha_1 + \dots + x_n\alpha_n = 0\).

\item[2.] Let
    \[\alpha_1 = (1, 1, -2, 1),\quad \alpha_2 = (3, 0, 4, -1),\quad \alpha_3 = (-1, 2, 5, 2).\]
    Let
    \[\alpha = (4, -5, 9, -7),\quad \beta = (3, 1, -4, 4),\quad \gamma = (-1, 1, 0, 1).\]

    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Which of the vectors \(\alpha, \beta, \gamma\) are in the subspace of \(R^4\) spanned by the \(\alpha_i\)?

            \[
                \begin{bmatrix}
                    1 & 1 & -2 & 1 \\
                    3 & 0 & 4 & -1 \\
                    -1 & 2 & 5 & 2
                \end{bmatrix}
                \rightarrow
                \begin{bmatrix}
                    1 & 0 & 0 & \frac{-3}{13} \\
                    0 & 1 & 0 & \frac{14}{13} \\
                    0 & 0 & 1 & \frac{-1}{13}
                \end{bmatrix}
            \]

            A basis of the subspace is \(\{(1, 0, 0, \frac{-3}{13}), (0, 1, 0, \frac{14}{13}), (0, 0, 1, \frac{-1}{13})\}\),
            so any vector in the subspace is in the form \((x_1, x_2, x_3, \frac{1}{13}(-3x_1 + 14x_2 - x_3))\).
            It can be found that only \(\alpha\) is in the subspace.

        \item[(b)] Which of the vectors \(\alpha, \beta, \gamma\) are in the subspace of \(C^4\) spanned by the \(\alpha_i\)?

            The matrix with the row vectors of \(\alpha_i\) still reduces into the same row-reduced echelon matrix and
            gives the same set of vectors as its basis. Again, only \(\alpha\) is in the subspace.

        \item[(c)] Does this suggest a theorem?

            Let \(S\) be a subfield of \(F\), and \(A\) be a set of vectors in \(S^n\) that spans a subspace of \(F^n\).
            Then the space spanned by \(A\) is also a subspace of \(S^n\).

    \end{enumerate}

\item[3.] Consider the vectors in \(\mathbb{R}^4\) defined by
    \[
        \alpha_1 = (-1, 0, 1, 2),\quad \alpha_2 = (3, 4, -2, 5),\quad \alpha_3 = (1, 4, 0, 9).
    \]
    Find a system of homogeneous linear equations for which the space of solutions is
    exactly the subspace of \(\mathbb{R}^4\) spanned by the given vectors.

    Reducing a matrix where the row vectors are given vectors shows that
    a basis of the given subspace is \(\{(1, 0, -1, -2), (0, 1, \frac{1}{4}, \frac{11}{4})\}\).
    So for any vector \((x_1, x_2, x_3, x_4)\) in the subspace,
    \[
        \begin{system}{2}
            -x_1 &+& \frac{1}{4}x_2 &=& x_3 \\
            -2x_1 &+& \frac{11}{4}x_2 &=& x_4 \\
        \end{system}
    \]
    \[
        \begin{system}{3}
            -x_1 &+& \frac{1}{4}x_2 &-& x_3 &=& 0 \\
            -2x_1 &+& \frac{11}{4}x_2 &-& x_4 &=& 0 \\
        \end{system}
    \]
    which is the desired system of equations.

\item[4.] In \(\mathbb{C}^3\), let
    \[
        \alpha_1 = (1, 0, -i),\quad \alpha_2 = (1 + i, 1 - i, 1),\quad \alpha_3 = (i, i, i).
    \]
    Prove that these vectors form a basis for \(\mathbb{C}^3\).
    What are the coordinates of the vector \((a, b, c)\) in this basis?

    A basic gist because I'm too lazy to write down the work: 

    Write a matrix with the three vectors as row vectors. Reduce it to see that it is row equivalent to a identity matrix
    so the three form a basis for \(\mathbb{C}^3\).

    The coordinates of the vector \((a, b, c)\) is \(P^{-1}\begin{bmatrix}a \\ b \\ c\end{bmatrix}\)
    where \(P\) is the matrix where the columns are the given basis vectors.

\item[6.] Let \(V\) be the real vector space spanned by the rows of the matrix
    \[
        A = \begin{bmatrix}
            3 & 21 & 0 & 9 & 0 \\
            1 & 7 & -1 & -2 & -1 \\
            2 & 14 & 0 & 6 & 1 \\
            6 & 42 & -1 & 13 & 0
        \end{bmatrix}.
    \]
    \begin{enumerate}[listparindent=\parindent]
        \item[(a)] Find a basis for \(V\).
            Row reduce the matrix and take the non-zero row vectors:
            \[
                \{ (1, 7, 0, 3, 0), (0, 0, 1, 5, 0), (0, 0, 0, 0, 1) \}
            \]
        \item[(b)] Tell which vectors \((x_1, x_2, x_3, x_4, x_5)\) are elements of \(V\).

            \[ (a_1, 7a_1, a_2, 3a_1 + 5a_2 + a_3) \]

        \item[(c)] If \((x_1, x_2, x_3, x_4, x_5)\) is in \(V\) what are its coordinates in the basis chosen in part (a)?

            \begin{gather*}
                (x_1, x_2 , x_3, x_4, x_5) = (a_1, 7a_1, a_2, 3a_1 + 5a_2 + a_3) \\
                (x_1, x_2 , x_3, x_4, x_5) = a_1(1, 7, 0, 3, 0) + a_2(0, 0, 1, 5, 0) + (0, 0, 0, 0, 1)
            \end{gather*}
            Therefore the coordinate vector is \(\begin{bmatrix}a_1 \\ a_2 \\ a_3\end{bmatrix}\).
    \end{enumerate}

\item[7.] Let \(A\) be a \(m \times m\) matrix over the field \(F\), and consider the system of equations \(AX = Y\).
    Prove that this system of equations has a solution if and only if the row rank of \(A\) is equal to the row rank
    of the augmented matrix of the system.
    
    Suppose there exists a solution to the system \(AX = Y\) and let \([R | Z]\) be the row-reduced echelon form of \([A | Y]\).

    The zero rows of \(R\) must be the last \(n\) rows, where \(n = m - \rank(A)\).
    Then the last \(n\) rows of \(Z\) must be zero rows as well,
    otherwise there is a row in the form \([0 \dots 0 | x], x \neq 0\) which contradicts the existence of a solution.
    Then \([R | Z]\) must also have precisely have \(n\) zero rows, so \(\rank(A) = \rank([A | Y])\).

    If there is no solution, there exists a row in the form \([0 \dots 0 | x], x \neq 0\) so \(\rank([A | Y]) > \rank(A)\).

\end{enumerate}
\end{document}
